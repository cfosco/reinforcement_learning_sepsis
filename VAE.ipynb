{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from vae import encoder, generator\n",
    "import keras.backend as K\n",
    "from keras.layers import Lambda, Dense as kDense, Multiply, PReLU, ZeroPadding1D, Input, Concatenate, Reshape\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.objectives import mean_squared_error\n",
    "from models_keras import MADE, _mask_matrix_made, Dense, _activation, gate, Conv1D, Dense\n",
    "import numpy as np\n",
    "from parse_dataset import *\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and parse dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('Sepsis_imp.csv')\n",
    "replace_absurd_temperatures(data)\n",
    "data = drop_patients_with_absurd_weights(data)\n",
    "data = drop_patients_with_unrealistic_HR_or_BP(data)\n",
    "data = add_relative_time_column(data)\n",
    "data = drop_patient_with_negative_input(data)\n",
    "add_small_quantities(data)\n",
    "create_action_column(data)\n",
    "add_log_actions(data)\n",
    "\n",
    "log_scaler = StandardScaler()\n",
    "scaler = StandardScaler()\n",
    "action_scaler = StandardScaler()\n",
    "train_idx, test_idx = split_train_test_idx(data)\n",
    "\n",
    "# scale on train data only\n",
    "scaler.fit(data.loc[data.icustayid.isin(train_idx)][numerical_columns_not_to_be_logged])\n",
    "log_scaler.fit(np.log(data.loc[data.icustayid.isin(train_idx)][numerical_columns_to_be_logged]))\n",
    "action_scaler.fit(data.loc[data.icustayid.isin(train_idx)][log_action_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_generator(data, last, idx, scaler, log_scaler, action_scaler, batch_size=32, log_action=True, verbose=False):\n",
    "    \"\"\"used in fit_generator\"\"\"\n",
    "    X_action_bin, X_features_bin, X_num, X_action, X_finished, X_alive = matrify(data, idx, scaler, log_scaler,action_scaler, verbose=False, log_action=log_action)\n",
    "    while True:\n",
    "        # shuffle\n",
    "        X_action_bin, X_features_bin, X_num, X_action, X_finished, X_alive = shuffle(X_action_bin, X_features_bin, X_num, X_action, X_finished, X_alive)\n",
    "\n",
    "        # take a part only\n",
    "        if last is not None:\n",
    "            X_action_bin, X_features_bin, X_num, X_action, X_finished, X_alive = X_action_bin[:last], X_features_bin[:last], X_num[:last], X_action[:last], X_finished[:last], X_alive[:last]\n",
    "\n",
    "        # one-hot encode\n",
    "        if verbose:\n",
    "            iterator = tqdm(range(0, X_num.shape[0], batch_size))\n",
    "        else:\n",
    "            iterator = range(0, X_num.shape[0], batch_size)\n",
    "        for i in iterator:\n",
    "            x_action_bin = X_action_bin[i:i+batch_size]\n",
    "            x_features_bin = X_features_bin[i:i+batch_size]\n",
    "            x_num = X_num[i:i+batch_size]\n",
    "            action = X_action[i:i+batch_size]\n",
    "            finished = X_finished[i:i+batch_size]\n",
    "            alive = X_alive[i:i+batch_size]\n",
    "            \n",
    "            if x_num.shape[0] != batch_size:\n",
    "                continue\n",
    "            yield [x_num, x_features_bin, x_action_bin, action], [x_num, finished, alive]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "E = encoder(num_filters=50)\n",
    "G = generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_z(batch_size, latent_dim, epsilon_std):\n",
    "    return lambda args: _sampling(args, batch_size, latent_dim, epsilon_std)\n",
    "\n",
    "def _sampling(args, batch_size, latent_dim, epsilon_std):\n",
    "    z_mean, z_var = args\n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0.,\n",
    "                              stddev=epsilon_std)\n",
    "    return z_mean + K.sqrt(z_var+1e-8) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "latent_dim = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x -> z\n",
    "vitals, binary_features, binary_actions, actions = E.inputs\n",
    "z_mean, z_var = E([vitals, binary_features, binary_actions, actions])  # we need those to compute KL divergence, so it cannot be directly included in the encoder function\n",
    "z = Lambda(sample_z(batch_size, latent_dim, 1.), output_shape=(latent_dim,))([z_mean, z_var])\n",
    "\n",
    "E = Model([vitals, binary_features, binary_actions, actions], z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_3 (InputLayer)             (None, 32, 45)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_1 (InputLayer)             (None, 2)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 32, 3)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_4 (InputLayer)             (None, 32, 2)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "model_1 (Model)                  [(None, 32), (None, 3 30686       input_3[0][0]                    \n",
      "                                                                   input_1[0][0]                    \n",
      "                                                                   input_2[0][0]                    \n",
      "                                                                   input_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)                (None, 32)            0           model_1[1][0]                    \n",
      "                                                                   model_1[1][1]                    \n",
      "====================================================================================================\n",
      "Total params: 30,686\n",
      "Trainable params: 30,284\n",
      "Non-trainable params: 402\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "E.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x -> z -> x'\n",
    "# update the encoder and generator weights on their ability to reconstruct the input protein\n",
    "vitals, binary_features, binary_actions, actions = E.inputs\n",
    "z = E([vitals, binary_features, binary_actions, actions])\n",
    "\n",
    "next_vitals, finished, dead_or_alive = G([vitals, binary_features, binary_actions, actions, z])\n",
    "VAE = Model([vitals, binary_features, binary_actions, actions], [next_vitals, finished, dead_or_alive])\n",
    "\n",
    "def _vae_loss(x, x_decoded_mean, z_mean, z_var):\n",
    "    mse = mean_squared_error(x, x_decoded_mean)\n",
    "    mse_loss = K.sum(mse, -1)\n",
    "    kl_loss = - 0.5 * K.sum(1 + K.log(z_var+1e-8) - K.square(z_mean) - z_var, axis=-1)\n",
    "#     return K.mean(mse_loss + kl_loss)\n",
    "    return mse_loss + kl_loss\n",
    "def vae_loss(z_mean, z_var):\n",
    "    return lambda x, x_decoded: _vae_loss(x, x_decoded, z_mean, z_var)\n",
    "\n",
    "VAE.compile(loss=[vae_loss(z_mean, z_var), 'binary_crossentropy', 'binary_crossentropy'], optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_3 (InputLayer)             (None, 32, 45)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_1 (InputLayer)             (None, 2)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 32, 3)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_4 (InputLayer)             (None, 32, 2)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "model_3 (Model)                  (None, 32)            30686       input_3[0][0]                    \n",
      "                                                                   input_1[0][0]                    \n",
      "                                                                   input_2[0][0]                    \n",
      "                                                                   input_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "model_2 (Model)                  [(None, 32, 45), (Non 65512       input_3[0][0]                    \n",
      "                                                                   input_1[0][0]                    \n",
      "                                                                   input_2[0][0]                    \n",
      "                                                                   input_4[0][0]                    \n",
      "                                                                   model_3[1][0]                    \n",
      "====================================================================================================\n",
      "Total params: 96,198\n",
      "Trainable params: 94,442\n",
      "Non-trainable params: 1,756\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "VAE.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 80/141 [================>.............] - ETA: 8586s - loss: 40.1307 - model_2_loss_1: 38.3853 - model_2_loss_2: 0.9231 - model_2_loss_3: 0.822 - ETA: 4291s - loss: 39.8566 - model_2_loss_1: 38.1234 - model_2_loss_2: 0.9153 - model_2_loss_3: 0.818 - ETA: 2859s - loss: 39.8386 - model_2_loss_1: 38.1382 - model_2_loss_2: 0.8900 - model_2_loss_3: 0.810 - ETA: 2143s - loss: 39.6810 - model_2_loss_1: 38.0028 - model_2_loss_2: 0.8695 - model_2_loss_3: 0.808 - ETA: 1713s - loss: 39.4325 - model_2_loss_1: 37.7767 - model_2_loss_2: 0.8497 - model_2_loss_3: 0.806 - ETA: 1426s - loss: 39.3303 - model_2_loss_1: 37.7029 - model_2_loss_2: 0.8266 - model_2_loss_3: 0.800 - ETA: 1221s - loss: 39.1403 - model_2_loss_1: 37.5320 - model_2_loss_2: 0.8087 - model_2_loss_3: 0.799 - ETA: 1067s - loss: 39.1620 - model_2_loss_1: 37.5759 - model_2_loss_2: 0.7916 - model_2_loss_3: 0.794 - ETA: 947s - loss: 39.0060 - model_2_loss_1: 37.4406 - model_2_loss_2: 0.7735 - model_2_loss_3: 0.791 - ETA: 851s - loss: 38.9698 - model_2_loss_1: 37.4230 - model_2_loss_2: 0.7567 - model_2_loss_3: 0.79 - ETA: 773s - loss: 38.9144 - model_2_loss_1: 37.3853 - model_2_loss_2: 0.7409 - model_2_loss_3: 0.78 - ETA: 707s - loss: 38.7390 - model_2_loss_1: 37.2262 - model_2_loss_2: 0.7264 - model_2_loss_3: 0.78 - ETA: 652s - loss: 38.6163 - model_2_loss_1: 37.1188 - model_2_loss_2: 0.7125 - model_2_loss_3: 0.78 - ETA: 604s - loss: 38.4825 - model_2_loss_1: 37.0008 - model_2_loss_2: 0.6992 - model_2_loss_3: 0.78 - ETA: 563s - loss: 38.3496 - model_2_loss_1: 36.8801 - model_2_loss_2: 0.6879 - model_2_loss_3: 0.78 - ETA: 527s - loss: 38.2889 - model_2_loss_1: 36.8310 - model_2_loss_2: 0.6777 - model_2_loss_3: 0.78 - ETA: 495s - loss: 38.1718 - model_2_loss_1: 36.7240 - model_2_loss_2: 0.6689 - model_2_loss_3: 0.77 - ETA: 466s - loss: 38.0669 - model_2_loss_1: 36.6296 - model_2_loss_2: 0.6599 - model_2_loss_3: 0.77 - ETA: 441s - loss: 37.9590 - model_2_loss_1: 36.5311 - model_2_loss_2: 0.6524 - model_2_loss_3: 0.77 - ETA: 418s - loss: 37.8593 - model_2_loss_1: 36.4404 - model_2_loss_2: 0.6448 - model_2_loss_3: 0.77 - ETA: 397s - loss: 37.7724 - model_2_loss_1: 36.3620 - model_2_loss_2: 0.6381 - model_2_loss_3: 0.77 - ETA: 378s - loss: 37.6751 - model_2_loss_1: 36.2731 - model_2_loss_2: 0.6315 - model_2_loss_3: 0.77 - ETA: 360s - loss: 37.5826 - model_2_loss_1: 36.1874 - model_2_loss_2: 0.6259 - model_2_loss_3: 0.76 - ETA: 344s - loss: 37.5099 - model_2_loss_1: 36.1224 - model_2_loss_2: 0.6200 - model_2_loss_3: 0.76 - ETA: 330s - loss: 37.4272 - model_2_loss_1: 36.0473 - model_2_loss_2: 0.6146 - model_2_loss_3: 0.76 - ETA: 316s - loss: 37.3475 - model_2_loss_1: 35.9734 - model_2_loss_2: 0.6097 - model_2_loss_3: 0.76 - ETA: 303s - loss: 37.2869 - model_2_loss_1: 35.9180 - model_2_loss_2: 0.6055 - model_2_loss_3: 0.76 - ETA: 292s - loss: 37.2054 - model_2_loss_1: 35.8429 - model_2_loss_2: 0.6007 - model_2_loss_3: 0.76 - ETA: 281s - loss: 37.1234 - model_2_loss_1: 35.7658 - model_2_loss_2: 0.5967 - model_2_loss_3: 0.76 - ETA: 270s - loss: 37.0460 - model_2_loss_1: 35.6939 - model_2_loss_2: 0.5926 - model_2_loss_3: 0.75 - ETA: 261s - loss: 36.9672 - model_2_loss_1: 35.6202 - model_2_loss_2: 0.5887 - model_2_loss_3: 0.75 - ETA: 252s - loss: 36.8889 - model_2_loss_1: 35.5474 - model_2_loss_2: 0.5850 - model_2_loss_3: 0.75 - ETA: 243s - loss: 36.8155 - model_2_loss_1: 35.4782 - model_2_loss_2: 0.5815 - model_2_loss_3: 0.75 - ETA: 235s - loss: 36.7334 - model_2_loss_1: 35.4003 - model_2_loss_2: 0.5785 - model_2_loss_3: 0.75 - ETA: 227s - loss: 36.7085 - model_2_loss_1: 35.3795 - model_2_loss_2: 0.5758 - model_2_loss_3: 0.75 - ETA: 220s - loss: 36.6402 - model_2_loss_1: 35.3151 - model_2_loss_2: 0.5729 - model_2_loss_3: 0.75 - ETA: 213s - loss: 36.5671 - model_2_loss_1: 35.2458 - model_2_loss_2: 0.5702 - model_2_loss_3: 0.75 - ETA: 207s - loss: 36.5123 - model_2_loss_1: 35.1940 - model_2_loss_2: 0.5677 - model_2_loss_3: 0.75 - ETA: 201s - loss: 36.4481 - model_2_loss_1: 35.1334 - model_2_loss_2: 0.5649 - model_2_loss_3: 0.74 - ETA: 195s - loss: 36.3758 - model_2_loss_1: 35.0652 - model_2_loss_2: 0.5621 - model_2_loss_3: 0.74 - ETA: 189s - loss: 36.3080 - model_2_loss_1: 35.0011 - model_2_loss_2: 0.5592 - model_2_loss_3: 0.74 - ETA: 183s - loss: 36.2391 - model_2_loss_1: 34.9352 - model_2_loss_2: 0.5571 - model_2_loss_3: 0.74 - ETA: 178s - loss: 36.1744 - model_2_loss_1: 34.8738 - model_2_loss_2: 0.5549 - model_2_loss_3: 0.74 - ETA: 173s - loss: 36.0989 - model_2_loss_1: 34.8015 - model_2_loss_2: 0.5527 - model_2_loss_3: 0.74 - ETA: 169s - loss: 36.0311 - model_2_loss_1: 34.7371 - model_2_loss_2: 0.5506 - model_2_loss_3: 0.74 - ETA: 164s - loss: 35.9715 - model_2_loss_1: 34.6803 - model_2_loss_2: 0.5485 - model_2_loss_3: 0.74 - ETA: 160s - loss: 35.9090 - model_2_loss_1: 34.6206 - model_2_loss_2: 0.5465 - model_2_loss_3: 0.74 - ETA: 155s - loss: 35.8478 - model_2_loss_1: 34.5624 - model_2_loss_2: 0.5444 - model_2_loss_3: 0.74 - ETA: 151s - loss: 35.7858 - model_2_loss_1: 34.5032 - model_2_loss_2: 0.5423 - model_2_loss_3: 0.74 - ETA: 147s - loss: 35.7314 - model_2_loss_1: 34.4513 - model_2_loss_2: 0.5404 - model_2_loss_3: 0.73 - ETA: 144s - loss: 35.6766 - model_2_loss_1: 34.3989 - model_2_loss_2: 0.5387 - model_2_loss_3: 0.73 - ETA: 140s - loss: 35.6174 - model_2_loss_1: 34.3423 - model_2_loss_2: 0.5370 - model_2_loss_3: 0.73 - ETA: 137s - loss: 35.5553 - model_2_loss_1: 34.2829 - model_2_loss_2: 0.5350 - model_2_loss_3: 0.73 - ETA: 133s - loss: 35.4916 - model_2_loss_1: 34.2218 - model_2_loss_2: 0.5333 - model_2_loss_3: 0.73 - ETA: 130s - loss: 35.4286 - model_2_loss_1: 34.1609 - model_2_loss_2: 0.5316 - model_2_loss_3: 0.73 - ETA: 127s - loss: 35.3702 - model_2_loss_1: 34.1046 - model_2_loss_2: 0.5302 - model_2_loss_3: 0.73 - ETA: 124s - loss: 35.3161 - model_2_loss_1: 34.0527 - model_2_loss_2: 0.5285 - model_2_loss_3: 0.73 - ETA: 120s - loss: 35.2555 - model_2_loss_1: 33.9942 - model_2_loss_2: 0.5268 - model_2_loss_3: 0.73 - ETA: 118s - loss: 35.1989 - model_2_loss_1: 33.9395 - model_2_loss_2: 0.5252 - model_2_loss_3: 0.73 - ETA: 115s - loss: 35.1401 - model_2_loss_1: 33.8829 - model_2_loss_2: 0.5236 - model_2_loss_3: 0.73 - ETA: 112s - loss: 35.0840 - model_2_loss_1: 33.8287 - model_2_loss_2: 0.5222 - model_2_loss_3: 0.73 - ETA: 109s - loss: 35.0246 - model_2_loss_1: 33.7717 - model_2_loss_2: 0.5208 - model_2_loss_3: 0.73 - ETA: 107s - loss: 34.9646 - model_2_loss_1: 33.7138 - model_2_loss_2: 0.5193 - model_2_loss_3: 0.73 - ETA: 104s - loss: 34.9218 - model_2_loss_1: 33.6733 - model_2_loss_2: 0.5179 - model_2_loss_3: 0.73 - ETA: 102s - loss: 34.8648 - model_2_loss_1: 33.6185 - model_2_loss_2: 0.5164 - model_2_loss_3: 0.72 - ETA: 99s - loss: 34.8051 - model_2_loss_1: 33.5608 - model_2_loss_2: 0.5151 - model_2_loss_3: 0.7292 - ETA: 97s - loss: 34.7474 - model_2_loss_1: 33.5050 - model_2_loss_2: 0.5138 - model_2_loss_3: 0.728 - ETA: 95s - loss: 34.6942 - model_2_loss_1: 33.4535 - model_2_loss_2: 0.5125 - model_2_loss_3: 0.728 - ETA: 92s - loss: 34.6398 - model_2_loss_1: 33.4012 - model_2_loss_2: 0.5111 - model_2_loss_3: 0.727 - ETA: 90s - loss: 34.5935 - model_2_loss_1: 33.3568 - model_2_loss_2: 0.5099 - model_2_loss_3: 0.726 - ETA: 88s - loss: 34.5433 - model_2_loss_1: 33.3081 - model_2_loss_2: 0.5087 - model_2_loss_3: 0.726 - ETA: 86s - loss: 34.4908 - model_2_loss_1: 33.2572 - model_2_loss_2: 0.5075 - model_2_loss_3: 0.726 - ETA: 84s - loss: 34.4416 - model_2_loss_1: 33.2097 - model_2_loss_2: 0.5064 - model_2_loss_3: 0.725 - ETA: 82s - loss: 34.3841 - model_2_loss_1: 33.1544 - model_2_loss_2: 0.5053 - model_2_loss_3: 0.724 - ETA: 80s - loss: 34.3287 - model_2_loss_1: 33.1007 - model_2_loss_2: 0.5041 - model_2_loss_3: 0.723 - ETA: 78s - loss: 34.2800 - model_2_loss_1: 33.0538 - model_2_loss_2: 0.5030 - model_2_loss_3: 0.723 - ETA: 76s - loss: 34.2263 - model_2_loss_1: 33.0015 - model_2_loss_2: 0.5019 - model_2_loss_3: 0.722 - ETA: 74s - loss: 34.1774 - model_2_loss_1: 32.9542 - model_2_loss_2: 0.5007 - model_2_loss_3: 0.722 - ETA: 73s - loss: 34.1249 - model_2_loss_1: 32.9034 - model_2_loss_2: 0.4996 - model_2_loss_3: 0.721 - ETA: 71s - loss: 34.0725 - model_2_loss_1: 32.8525 - model_2_loss_2: 0.4985 - model_2_loss_3: 0.721141/141 [==============================] - ETA: 69s - loss: 34.0322 - model_2_loss_1: 32.8137 - model_2_loss_2: 0.4975 - model_2_loss_3: 0.721 - ETA: 67s - loss: 33.9792 - model_2_loss_1: 32.7623 - model_2_loss_2: 0.4965 - model_2_loss_3: 0.720 - ETA: 66s - loss: 33.9273 - model_2_loss_1: 32.7120 - model_2_loss_2: 0.4954 - model_2_loss_3: 0.720 - ETA: 64s - loss: 33.8741 - model_2_loss_1: 32.6604 - model_2_loss_2: 0.4944 - model_2_loss_3: 0.719 - ETA: 62s - loss: 33.8254 - model_2_loss_1: 32.6132 - model_2_loss_2: 0.4935 - model_2_loss_3: 0.718 - ETA: 61s - loss: 33.7719 - model_2_loss_1: 32.5611 - model_2_loss_2: 0.4925 - model_2_loss_3: 0.718 - ETA: 59s - loss: 33.7529 - model_2_loss_1: 32.5437 - model_2_loss_2: 0.4917 - model_2_loss_3: 0.717 - ETA: 58s - loss: 33.7031 - model_2_loss_1: 32.4954 - model_2_loss_2: 0.4907 - model_2_loss_3: 0.717 - ETA: 56s - loss: 33.6529 - model_2_loss_1: 32.4467 - model_2_loss_2: 0.4897 - model_2_loss_3: 0.716 - ETA: 55s - loss: 33.6016 - model_2_loss_1: 32.3968 - model_2_loss_2: 0.4887 - model_2_loss_3: 0.716 - ETA: 53s - loss: 33.5551 - model_2_loss_1: 32.3520 - model_2_loss_2: 0.4877 - model_2_loss_3: 0.715 - ETA: 52s - loss: 33.5069 - model_2_loss_1: 32.3055 - model_2_loss_2: 0.4867 - model_2_loss_3: 0.714 - ETA: 51s - loss: 33.4590 - model_2_loss_1: 32.2589 - model_2_loss_2: 0.4858 - model_2_loss_3: 0.714 - ETA: 49s - loss: 33.4122 - model_2_loss_1: 32.2137 - model_2_loss_2: 0.4849 - model_2_loss_3: 0.713 - ETA: 48s - loss: 33.3656 - model_2_loss_1: 32.1684 - model_2_loss_2: 0.4840 - model_2_loss_3: 0.713 - ETA: 46s - loss: 33.3250 - model_2_loss_1: 32.1293 - model_2_loss_2: 0.4831 - model_2_loss_3: 0.712 - ETA: 45s - loss: 33.2772 - model_2_loss_1: 32.0830 - model_2_loss_2: 0.4823 - model_2_loss_3: 0.711 - ETA: 44s - loss: 33.2273 - model_2_loss_1: 32.0345 - model_2_loss_2: 0.4814 - model_2_loss_3: 0.711 - ETA: 43s - loss: 33.1824 - model_2_loss_1: 31.9909 - model_2_loss_2: 0.4806 - model_2_loss_3: 0.710 - ETA: 41s - loss: 33.1343 - model_2_loss_1: 31.9444 - model_2_loss_2: 0.4797 - model_2_loss_3: 0.710 - ETA: 40s - loss: 33.0879 - model_2_loss_1: 31.8995 - model_2_loss_2: 0.4789 - model_2_loss_3: 0.709 - ETA: 39s - loss: 33.0439 - model_2_loss_1: 31.8567 - model_2_loss_2: 0.4781 - model_2_loss_3: 0.709 - ETA: 38s - loss: 32.9963 - model_2_loss_1: 31.8104 - model_2_loss_2: 0.4772 - model_2_loss_3: 0.708 - ETA: 36s - loss: 32.9515 - model_2_loss_1: 31.7671 - model_2_loss_2: 0.4764 - model_2_loss_3: 0.708 - ETA: 35s - loss: 32.9046 - model_2_loss_1: 31.7214 - model_2_loss_2: 0.4756 - model_2_loss_3: 0.707 - ETA: 34s - loss: 32.8576 - model_2_loss_1: 31.6755 - model_2_loss_2: 0.4748 - model_2_loss_3: 0.707 - ETA: 33s - loss: 32.8138 - model_2_loss_1: 31.6329 - model_2_loss_2: 0.4740 - model_2_loss_3: 0.706 - ETA: 32s - loss: 32.7690 - model_2_loss_1: 31.5892 - model_2_loss_2: 0.4733 - model_2_loss_3: 0.706 - ETA: 31s - loss: 32.7251 - model_2_loss_1: 31.5466 - model_2_loss_2: 0.4726 - model_2_loss_3: 0.706 - ETA: 29s - loss: 32.6814 - model_2_loss_1: 31.5038 - model_2_loss_2: 0.4718 - model_2_loss_3: 0.705 - ETA: 28s - loss: 32.6398 - model_2_loss_1: 31.4632 - model_2_loss_2: 0.4711 - model_2_loss_3: 0.705 - ETA: 27s - loss: 32.5918 - model_2_loss_1: 31.4164 - model_2_loss_2: 0.4704 - model_2_loss_3: 0.705 - ETA: 26s - loss: 32.5482 - model_2_loss_1: 31.3740 - model_2_loss_2: 0.4696 - model_2_loss_3: 0.704 - ETA: 25s - loss: 32.5054 - model_2_loss_1: 31.3324 - model_2_loss_2: 0.4688 - model_2_loss_3: 0.704 - ETA: 24s - loss: 32.4639 - model_2_loss_1: 31.2919 - model_2_loss_2: 0.4681 - model_2_loss_3: 0.704 - ETA: 23s - loss: 32.4228 - model_2_loss_1: 31.2521 - model_2_loss_2: 0.4674 - model_2_loss_3: 0.703 - ETA: 22s - loss: 32.3796 - model_2_loss_1: 31.2099 - model_2_loss_2: 0.4667 - model_2_loss_3: 0.703 - ETA: 21s - loss: 32.3359 - model_2_loss_1: 31.1673 - model_2_loss_2: 0.4660 - model_2_loss_3: 0.702 - ETA: 20s - loss: 32.2948 - model_2_loss_1: 31.1273 - model_2_loss_2: 0.4653 - model_2_loss_3: 0.702 - ETA: 19s - loss: 32.2530 - model_2_loss_1: 31.0867 - model_2_loss_2: 0.4647 - model_2_loss_3: 0.701 - ETA: 18s - loss: 32.2143 - model_2_loss_1: 31.0491 - model_2_loss_2: 0.4640 - model_2_loss_3: 0.701 - ETA: 17s - loss: 32.1745 - model_2_loss_1: 31.0104 - model_2_loss_2: 0.4633 - model_2_loss_3: 0.700 - ETA: 16s - loss: 32.1318 - model_2_loss_1: 30.9688 - model_2_loss_2: 0.4626 - model_2_loss_3: 0.700 - ETA: 15s - loss: 32.0923 - model_2_loss_1: 30.9302 - model_2_loss_2: 0.4620 - model_2_loss_3: 0.700 - ETA: 14s - loss: 32.0523 - model_2_loss_1: 30.8911 - model_2_loss_2: 0.4613 - model_2_loss_3: 0.699 - ETA: 13s - loss: 32.0121 - model_2_loss_1: 30.8518 - model_2_loss_2: 0.4607 - model_2_loss_3: 0.699 - ETA: 12s - loss: 31.9710 - model_2_loss_1: 30.8118 - model_2_loss_2: 0.4601 - model_2_loss_3: 0.699 - ETA: 11s - loss: 31.9297 - model_2_loss_1: 30.7713 - model_2_loss_2: 0.4595 - model_2_loss_3: 0.698 - ETA: 10s - loss: 31.8887 - model_2_loss_1: 30.7314 - model_2_loss_2: 0.4589 - model_2_loss_3: 0.698 - ETA: 9s - loss: 31.8471 - model_2_loss_1: 30.6908 - model_2_loss_2: 0.4583 - model_2_loss_3: 0.698 - ETA: 8s - loss: 31.8059 - model_2_loss_1: 30.6506 - model_2_loss_2: 0.4577 - model_2_loss_3: 0.69 - ETA: 7s - loss: 31.7741 - model_2_loss_1: 30.6197 - model_2_loss_2: 0.4571 - model_2_loss_3: 0.69 - ETA: 6s - loss: 31.7335 - model_2_loss_1: 30.5800 - model_2_loss_2: 0.4565 - model_2_loss_3: 0.69 - ETA: 6s - loss: 31.6928 - model_2_loss_1: 30.5403 - model_2_loss_2: 0.4559 - model_2_loss_3: 0.69 - ETA: 5s - loss: 31.6523 - model_2_loss_1: 30.5008 - model_2_loss_2: 0.4553 - model_2_loss_3: 0.69 - ETA: 4s - loss: 31.6126 - model_2_loss_1: 30.4622 - model_2_loss_2: 0.4547 - model_2_loss_3: 0.69 - ETA: 3s - loss: 31.5715 - model_2_loss_1: 30.4219 - model_2_loss_2: 0.4542 - model_2_loss_3: 0.69 - ETA: 2s - loss: 31.5320 - model_2_loss_1: 30.3835 - model_2_loss_2: 0.4536 - model_2_loss_3: 0.69 - ETA: 1s - loss: 31.4939 - model_2_loss_1: 30.3463 - model_2_loss_2: 0.4530 - model_2_loss_3: 0.69 - ETA: 0s - loss: 31.4584 - model_2_loss_1: 30.3117 - model_2_loss_2: 0.4525 - model_2_loss_3: 0.69 - 123s - loss: 31.4192 - model_2_loss_1: 30.2736 - model_2_loss_2: 0.4519 - model_2_loss_3: 0.6938 - val_loss: 20.9958 - val_model_2_loss_1: 20.0084 - val_model_2_loss_2: 0.3596 - val_model_2_loss_3: 0.6279\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 80/141 [================>.............] - ETA: 58s - loss: 25.7925 - model_2_loss_1: 24.7725 - model_2_loss_2: 0.3707 - model_2_loss_3: 0.649 - ETA: 61s - loss: 25.8025 - model_2_loss_1: 24.7915 - model_2_loss_2: 0.3719 - model_2_loss_3: 0.639 - ETA: 60s - loss: 25.8254 - model_2_loss_1: 24.8098 - model_2_loss_2: 0.3720 - model_2_loss_3: 0.643 - ETA: 62s - loss: 25.9832 - model_2_loss_1: 24.9703 - model_2_loss_2: 0.3736 - model_2_loss_3: 0.639 - ETA: 63s - loss: 26.0243 - model_2_loss_1: 25.0106 - model_2_loss_2: 0.3744 - model_2_loss_3: 0.639 - ETA: 61s - loss: 25.9710 - model_2_loss_1: 24.9587 - model_2_loss_2: 0.3743 - model_2_loss_3: 0.638 - ETA: 60s - loss: 25.9502 - model_2_loss_1: 24.9379 - model_2_loss_2: 0.3747 - model_2_loss_3: 0.637 - ETA: 58s - loss: 25.9006 - model_2_loss_1: 24.8884 - model_2_loss_2: 0.3746 - model_2_loss_3: 0.637 - ETA: 57s - loss: 25.8590 - model_2_loss_1: 24.8459 - model_2_loss_2: 0.3741 - model_2_loss_3: 0.639 - ETA: 56s - loss: 25.8184 - model_2_loss_1: 24.8078 - model_2_loss_2: 0.3740 - model_2_loss_3: 0.636 - ETA: 56s - loss: 25.7732 - model_2_loss_1: 24.7630 - model_2_loss_2: 0.3733 - model_2_loss_3: 0.636 - ETA: 55s - loss: 25.7443 - model_2_loss_1: 24.7342 - model_2_loss_2: 0.3727 - model_2_loss_3: 0.637 - ETA: 54s - loss: 25.6932 - model_2_loss_1: 24.6839 - model_2_loss_2: 0.3726 - model_2_loss_3: 0.636 - ETA: 54s - loss: 25.6803 - model_2_loss_1: 24.6719 - model_2_loss_2: 0.3724 - model_2_loss_3: 0.636 - ETA: 53s - loss: 25.6328 - model_2_loss_1: 24.6254 - model_2_loss_2: 0.3720 - model_2_loss_3: 0.635 - ETA: 53s - loss: 25.6147 - model_2_loss_1: 24.6085 - model_2_loss_2: 0.3715 - model_2_loss_3: 0.634 - ETA: 52s - loss: 25.6044 - model_2_loss_1: 24.5988 - model_2_loss_2: 0.3715 - model_2_loss_3: 0.634 - ETA: 52s - loss: 25.5652 - model_2_loss_1: 24.5600 - model_2_loss_2: 0.3714 - model_2_loss_3: 0.633 - ETA: 51s - loss: 25.5281 - model_2_loss_1: 24.5230 - model_2_loss_2: 0.3710 - model_2_loss_3: 0.634 - ETA: 51s - loss: 25.4921 - model_2_loss_1: 24.4875 - model_2_loss_2: 0.3707 - model_2_loss_3: 0.633 - ETA: 50s - loss: 25.4798 - model_2_loss_1: 24.4749 - model_2_loss_2: 0.3702 - model_2_loss_3: 0.634 - ETA: 50s - loss: 25.4450 - model_2_loss_1: 24.4398 - model_2_loss_2: 0.3700 - model_2_loss_3: 0.635 - ETA: 49s - loss: 25.4170 - model_2_loss_1: 24.4129 - model_2_loss_2: 0.3699 - model_2_loss_3: 0.634 - ETA: 49s - loss: 25.3847 - model_2_loss_1: 24.3809 - model_2_loss_2: 0.3696 - model_2_loss_3: 0.634 - ETA: 48s - loss: 25.3405 - model_2_loss_1: 24.3368 - model_2_loss_2: 0.3696 - model_2_loss_3: 0.634 - ETA: 48s - loss: 25.3237 - model_2_loss_1: 24.3203 - model_2_loss_2: 0.3692 - model_2_loss_3: 0.634 - ETA: 48s - loss: 25.2814 - model_2_loss_1: 24.2789 - model_2_loss_2: 0.3693 - model_2_loss_3: 0.633 - ETA: 47s - loss: 25.2537 - model_2_loss_1: 24.2509 - model_2_loss_2: 0.3689 - model_2_loss_3: 0.633 - ETA: 47s - loss: 25.2192 - model_2_loss_1: 24.2174 - model_2_loss_2: 0.3687 - model_2_loss_3: 0.633 - ETA: 46s - loss: 25.1825 - model_2_loss_1: 24.1810 - model_2_loss_2: 0.3686 - model_2_loss_3: 0.632 - ETA: 46s - loss: 25.1669 - model_2_loss_1: 24.1659 - model_2_loss_2: 0.3685 - model_2_loss_3: 0.632 - ETA: 45s - loss: 25.1368 - model_2_loss_1: 24.1358 - model_2_loss_2: 0.3685 - model_2_loss_3: 0.632 - ETA: 45s - loss: 25.1071 - model_2_loss_1: 24.1069 - model_2_loss_2: 0.3684 - model_2_loss_3: 0.631 - ETA: 45s - loss: 25.0776 - model_2_loss_1: 24.0776 - model_2_loss_2: 0.3679 - model_2_loss_3: 0.632 - ETA: 44s - loss: 25.0435 - model_2_loss_1: 24.0441 - model_2_loss_2: 0.3679 - model_2_loss_3: 0.631 - ETA: 44s - loss: 25.0066 - model_2_loss_1: 24.0073 - model_2_loss_2: 0.3676 - model_2_loss_3: 0.631 - ETA: 44s - loss: 24.9867 - model_2_loss_1: 23.9877 - model_2_loss_2: 0.3674 - model_2_loss_3: 0.631 - ETA: 44s - loss: 24.9487 - model_2_loss_1: 23.9504 - model_2_loss_2: 0.3672 - model_2_loss_3: 0.631 - ETA: 43s - loss: 24.9240 - model_2_loss_1: 23.9265 - model_2_loss_2: 0.3670 - model_2_loss_3: 0.630 - ETA: 43s - loss: 24.8907 - model_2_loss_1: 23.8931 - model_2_loss_2: 0.3668 - model_2_loss_3: 0.630 - ETA: 43s - loss: 24.8866 - model_2_loss_1: 23.8896 - model_2_loss_2: 0.3667 - model_2_loss_3: 0.630 - ETA: 42s - loss: 24.8631 - model_2_loss_1: 23.8668 - model_2_loss_2: 0.3665 - model_2_loss_3: 0.629 - ETA: 42s - loss: 24.8460 - model_2_loss_1: 23.8497 - model_2_loss_2: 0.3663 - model_2_loss_3: 0.630 - ETA: 42s - loss: 24.8125 - model_2_loss_1: 23.8167 - model_2_loss_2: 0.3661 - model_2_loss_3: 0.629 - ETA: 41s - loss: 24.7796 - model_2_loss_1: 23.7840 - model_2_loss_2: 0.3659 - model_2_loss_3: 0.629 - ETA: 41s - loss: 24.7465 - model_2_loss_1: 23.7511 - model_2_loss_2: 0.3657 - model_2_loss_3: 0.629 - ETA: 40s - loss: 24.7344 - model_2_loss_1: 23.7393 - model_2_loss_2: 0.3655 - model_2_loss_3: 0.629 - ETA: 40s - loss: 24.7066 - model_2_loss_1: 23.7118 - model_2_loss_2: 0.3654 - model_2_loss_3: 0.629 - ETA: 39s - loss: 24.6780 - model_2_loss_1: 23.6840 - model_2_loss_2: 0.3652 - model_2_loss_3: 0.628 - ETA: 39s - loss: 24.6462 - model_2_loss_1: 23.6527 - model_2_loss_2: 0.3650 - model_2_loss_3: 0.628 - ETA: 38s - loss: 24.6158 - model_2_loss_1: 23.6227 - model_2_loss_2: 0.3649 - model_2_loss_3: 0.628 - ETA: 38s - loss: 24.5892 - model_2_loss_1: 23.5964 - model_2_loss_2: 0.3646 - model_2_loss_3: 0.628 - ETA: 37s - loss: 24.5617 - model_2_loss_1: 23.5697 - model_2_loss_2: 0.3645 - model_2_loss_3: 0.627 - ETA: 37s - loss: 24.5343 - model_2_loss_1: 23.5428 - model_2_loss_2: 0.3643 - model_2_loss_3: 0.627 - ETA: 36s - loss: 24.5055 - model_2_loss_1: 23.5144 - model_2_loss_2: 0.3641 - model_2_loss_3: 0.627 - ETA: 36s - loss: 24.4729 - model_2_loss_1: 23.4823 - model_2_loss_2: 0.3639 - model_2_loss_3: 0.626 - ETA: 36s - loss: 24.4545 - model_2_loss_1: 23.4642 - model_2_loss_2: 0.3637 - model_2_loss_3: 0.626 - ETA: 35s - loss: 24.4343 - model_2_loss_1: 23.4450 - model_2_loss_2: 0.3635 - model_2_loss_3: 0.625 - ETA: 35s - loss: 24.4118 - model_2_loss_1: 23.4227 - model_2_loss_2: 0.3633 - model_2_loss_3: 0.625 - ETA: 35s - loss: 24.3795 - model_2_loss_1: 23.3909 - model_2_loss_2: 0.3631 - model_2_loss_3: 0.625 - ETA: 34s - loss: 24.3488 - model_2_loss_1: 23.3604 - model_2_loss_2: 0.3630 - model_2_loss_3: 0.625 - ETA: 34s - loss: 24.3205 - model_2_loss_1: 23.3324 - model_2_loss_2: 0.3629 - model_2_loss_3: 0.625 - ETA: 34s - loss: 24.2968 - model_2_loss_1: 23.3086 - model_2_loss_2: 0.3627 - model_2_loss_3: 0.625 - ETA: 33s - loss: 24.2678 - model_2_loss_1: 23.2802 - model_2_loss_2: 0.3626 - model_2_loss_3: 0.625 - ETA: 33s - loss: 24.2408 - model_2_loss_1: 23.2539 - model_2_loss_2: 0.3624 - model_2_loss_3: 0.624 - ETA: 32s - loss: 24.2180 - model_2_loss_1: 23.2314 - model_2_loss_2: 0.3623 - model_2_loss_3: 0.624 - ETA: 32s - loss: 24.1975 - model_2_loss_1: 23.2111 - model_2_loss_2: 0.3621 - model_2_loss_3: 0.624 - ETA: 31s - loss: 24.1719 - model_2_loss_1: 23.1856 - model_2_loss_2: 0.3619 - model_2_loss_3: 0.624 - ETA: 31s - loss: 24.1493 - model_2_loss_1: 23.1634 - model_2_loss_2: 0.3619 - model_2_loss_3: 0.624 - ETA: 31s - loss: 24.1211 - model_2_loss_1: 23.1359 - model_2_loss_2: 0.3617 - model_2_loss_3: 0.623 - ETA: 30s - loss: 24.0900 - model_2_loss_1: 23.1047 - model_2_loss_2: 0.3616 - model_2_loss_3: 0.623 - ETA: 30s - loss: 24.0634 - model_2_loss_1: 23.0784 - model_2_loss_2: 0.3615 - model_2_loss_3: 0.623 - ETA: 29s - loss: 24.0395 - model_2_loss_1: 23.0549 - model_2_loss_2: 0.3613 - model_2_loss_3: 0.623 - ETA: 29s - loss: 24.0118 - model_2_loss_1: 23.0275 - model_2_loss_2: 0.3611 - model_2_loss_3: 0.623 - ETA: 28s - loss: 23.9849 - model_2_loss_1: 23.0011 - model_2_loss_2: 0.3610 - model_2_loss_3: 0.622 - ETA: 28s - loss: 23.9585 - model_2_loss_1: 22.9749 - model_2_loss_2: 0.3608 - model_2_loss_3: 0.622 - ETA: 28s - loss: 23.9279 - model_2_loss_1: 22.9447 - model_2_loss_2: 0.3606 - model_2_loss_3: 0.622 - ETA: 27s - loss: 23.9037 - model_2_loss_1: 22.9209 - model_2_loss_2: 0.3604 - model_2_loss_3: 0.622 - ETA: 27s - loss: 23.8766 - model_2_loss_1: 22.8944 - model_2_loss_2: 0.3602 - model_2_loss_3: 0.622 - ETA: 26s - loss: 23.8502 - model_2_loss_1: 22.8682 - model_2_loss_2: 0.3600 - model_2_loss_3: 0.6220"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/141 [==============================] - ETA: 26s - loss: 23.8250 - model_2_loss_1: 22.8433 - model_2_loss_2: 0.3598 - model_2_loss_3: 0.621 - ETA: 25s - loss: 23.7994 - model_2_loss_1: 22.8182 - model_2_loss_2: 0.3597 - model_2_loss_3: 0.621 - ETA: 25s - loss: 23.7713 - model_2_loss_1: 22.7904 - model_2_loss_2: 0.3596 - model_2_loss_3: 0.621 - ETA: 24s - loss: 23.7445 - model_2_loss_1: 22.7640 - model_2_loss_2: 0.3593 - model_2_loss_3: 0.621 - ETA: 24s - loss: 23.7216 - model_2_loss_1: 22.7415 - model_2_loss_2: 0.3592 - model_2_loss_3: 0.620 - ETA: 24s - loss: 23.6942 - model_2_loss_1: 22.7145 - model_2_loss_2: 0.3590 - model_2_loss_3: 0.620 - ETA: 23s - loss: 23.6684 - model_2_loss_1: 22.6894 - model_2_loss_2: 0.3588 - model_2_loss_3: 0.620 - ETA: 23s - loss: 23.6401 - model_2_loss_1: 22.6618 - model_2_loss_2: 0.3586 - model_2_loss_3: 0.619 - ETA: 22s - loss: 23.6171 - model_2_loss_1: 22.6393 - model_2_loss_2: 0.3583 - model_2_loss_3: 0.619 - ETA: 22s - loss: 23.5905 - model_2_loss_1: 22.6129 - model_2_loss_2: 0.3581 - model_2_loss_3: 0.619 - ETA: 21s - loss: 23.5647 - model_2_loss_1: 22.5871 - model_2_loss_2: 0.3579 - model_2_loss_3: 0.619 - ETA: 21s - loss: 23.5459 - model_2_loss_1: 22.5687 - model_2_loss_2: 0.3578 - model_2_loss_3: 0.619 - ETA: 20s - loss: 23.5302 - model_2_loss_1: 22.5534 - model_2_loss_2: 0.3576 - model_2_loss_3: 0.619 - ETA: 20s - loss: 23.5041 - model_2_loss_1: 22.5278 - model_2_loss_2: 0.3574 - model_2_loss_3: 0.618 - ETA: 19s - loss: 23.4779 - model_2_loss_1: 22.5020 - model_2_loss_2: 0.3572 - model_2_loss_3: 0.618 - ETA: 19s - loss: 23.4537 - model_2_loss_1: 22.4781 - model_2_loss_2: 0.3570 - model_2_loss_3: 0.618 - ETA: 19s - loss: 23.4270 - model_2_loss_1: 22.4518 - model_2_loss_2: 0.3569 - model_2_loss_3: 0.618 - ETA: 18s - loss: 23.4024 - model_2_loss_1: 22.4275 - model_2_loss_2: 0.3567 - model_2_loss_3: 0.618 - ETA: 18s - loss: 23.3932 - model_2_loss_1: 22.4186 - model_2_loss_2: 0.3567 - model_2_loss_3: 0.617 - ETA: 17s - loss: 23.3676 - model_2_loss_1: 22.3934 - model_2_loss_2: 0.3565 - model_2_loss_3: 0.617 - ETA: 17s - loss: 23.3423 - model_2_loss_1: 22.3686 - model_2_loss_2: 0.3564 - model_2_loss_3: 0.617 - ETA: 16s - loss: 23.3168 - model_2_loss_1: 22.3432 - model_2_loss_2: 0.3562 - model_2_loss_3: 0.617 - ETA: 16s - loss: 23.2908 - model_2_loss_1: 22.3175 - model_2_loss_2: 0.3560 - model_2_loss_3: 0.617 - ETA: 16s - loss: 23.2658 - model_2_loss_1: 22.2929 - model_2_loss_2: 0.3558 - model_2_loss_3: 0.617 - ETA: 15s - loss: 23.2414 - model_2_loss_1: 22.2688 - model_2_loss_2: 0.3557 - model_2_loss_3: 0.616 - ETA: 15s - loss: 23.2159 - model_2_loss_1: 22.2436 - model_2_loss_2: 0.3555 - model_2_loss_3: 0.616 - ETA: 14s - loss: 23.1915 - model_2_loss_1: 22.2196 - model_2_loss_2: 0.3553 - model_2_loss_3: 0.616 - ETA: 14s - loss: 23.1628 - model_2_loss_1: 22.1913 - model_2_loss_2: 0.3552 - model_2_loss_3: 0.616 - ETA: 13s - loss: 23.1380 - model_2_loss_1: 22.1667 - model_2_loss_2: 0.3550 - model_2_loss_3: 0.616 - ETA: 13s - loss: 23.1124 - model_2_loss_1: 22.1416 - model_2_loss_2: 0.3549 - model_2_loss_3: 0.615 - ETA: 12s - loss: 23.0917 - model_2_loss_1: 22.1212 - model_2_loss_2: 0.3547 - model_2_loss_3: 0.615 - ETA: 12s - loss: 23.0693 - model_2_loss_1: 22.0992 - model_2_loss_2: 0.3546 - model_2_loss_3: 0.615 - ETA: 12s - loss: 23.0446 - model_2_loss_1: 22.0748 - model_2_loss_2: 0.3545 - model_2_loss_3: 0.615 - ETA: 11s - loss: 23.0219 - model_2_loss_1: 22.0525 - model_2_loss_2: 0.3543 - model_2_loss_3: 0.615 - ETA: 11s - loss: 22.9993 - model_2_loss_1: 22.0303 - model_2_loss_2: 0.3542 - model_2_loss_3: 0.614 - ETA: 10s - loss: 22.9783 - model_2_loss_1: 22.0098 - model_2_loss_2: 0.3541 - model_2_loss_3: 0.614 - ETA: 10s - loss: 22.9526 - model_2_loss_1: 21.9844 - model_2_loss_2: 0.3539 - model_2_loss_3: 0.614 - ETA: 9s - loss: 22.9263 - model_2_loss_1: 21.9585 - model_2_loss_2: 0.3538 - model_2_loss_3: 0.614 - ETA: 9s - loss: 22.9030 - model_2_loss_1: 21.9355 - model_2_loss_2: 0.3537 - model_2_loss_3: 0.61 - ETA: 9s - loss: 22.8794 - model_2_loss_1: 21.9122 - model_2_loss_2: 0.3535 - model_2_loss_3: 0.61 - ETA: 8s - loss: 22.8557 - model_2_loss_1: 21.8887 - model_2_loss_2: 0.3533 - model_2_loss_3: 0.61 - ETA: 8s - loss: 22.8304 - model_2_loss_1: 21.8637 - model_2_loss_2: 0.3532 - model_2_loss_3: 0.61 - ETA: 7s - loss: 22.8057 - model_2_loss_1: 21.8393 - model_2_loss_2: 0.3530 - model_2_loss_3: 0.61 - ETA: 7s - loss: 22.7816 - model_2_loss_1: 21.8155 - model_2_loss_2: 0.3528 - model_2_loss_3: 0.61 - ETA: 6s - loss: 22.7555 - model_2_loss_1: 21.7896 - model_2_loss_2: 0.3527 - model_2_loss_3: 0.61 - ETA: 6s - loss: 22.7306 - model_2_loss_1: 21.7649 - model_2_loss_2: 0.3526 - model_2_loss_3: 0.61 - ETA: 5s - loss: 22.7107 - model_2_loss_1: 21.7455 - model_2_loss_2: 0.3524 - model_2_loss_3: 0.61 - ETA: 5s - loss: 22.6882 - model_2_loss_1: 21.7233 - model_2_loss_2: 0.3523 - model_2_loss_3: 0.61 - ETA: 5s - loss: 22.6632 - model_2_loss_1: 21.6986 - model_2_loss_2: 0.3521 - model_2_loss_3: 0.61 - ETA: 4s - loss: 22.6436 - model_2_loss_1: 21.6794 - model_2_loss_2: 0.3520 - model_2_loss_3: 0.61 - ETA: 4s - loss: 22.6201 - model_2_loss_1: 21.6562 - model_2_loss_2: 0.3519 - model_2_loss_3: 0.61 - ETA: 3s - loss: 22.5968 - model_2_loss_1: 21.6333 - model_2_loss_2: 0.3518 - model_2_loss_3: 0.61 - ETA: 3s - loss: 22.5723 - model_2_loss_1: 21.6091 - model_2_loss_2: 0.3517 - model_2_loss_3: 0.61 - ETA: 2s - loss: 22.5491 - model_2_loss_1: 21.5862 - model_2_loss_2: 0.3515 - model_2_loss_3: 0.61 - ETA: 2s - loss: 22.5239 - model_2_loss_1: 21.5613 - model_2_loss_2: 0.3513 - model_2_loss_3: 0.61 - ETA: 2s - loss: 22.5009 - model_2_loss_1: 21.5388 - model_2_loss_2: 0.3512 - model_2_loss_3: 0.61 - ETA: 1s - loss: 22.4799 - model_2_loss_1: 21.5181 - model_2_loss_2: 0.3511 - model_2_loss_3: 0.61 - ETA: 1s - loss: 22.4558 - model_2_loss_1: 21.4945 - model_2_loss_2: 0.3509 - model_2_loss_3: 0.61 - ETA: 0s - loss: 22.4308 - model_2_loss_1: 21.4698 - model_2_loss_2: 0.3508 - model_2_loss_3: 0.61 - ETA: 0s - loss: 22.4074 - model_2_loss_1: 21.4467 - model_2_loss_2: 0.3506 - model_2_loss_3: 0.61 - 61s - loss: 22.3833 - model_2_loss_1: 21.4231 - model_2_loss_2: 0.3505 - model_2_loss_3: 0.6097 - val_loss: 15.1252 - val_model_2_loss_1: 14.2420 - val_model_2_loss_2: 0.3145 - val_model_2_loss_3: 0.5687\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 80/141 [================>.............] - ETA: 55s - loss: 19.1483 - model_2_loss_1: 18.2344 - model_2_loss_2: 0.3268 - model_2_loss_3: 0.587 - ETA: 57s - loss: 18.9533 - model_2_loss_1: 18.0439 - model_2_loss_2: 0.3281 - model_2_loss_3: 0.581 - ETA: 55s - loss: 18.9616 - model_2_loss_1: 18.0438 - model_2_loss_2: 0.3287 - model_2_loss_3: 0.589 - ETA: 55s - loss: 19.0219 - model_2_loss_1: 18.1065 - model_2_loss_2: 0.3286 - model_2_loss_3: 0.586 - ETA: 55s - loss: 19.0033 - model_2_loss_1: 18.0925 - model_2_loss_2: 0.3284 - model_2_loss_3: 0.582 - ETA: 55s - loss: 19.0603 - model_2_loss_1: 18.1477 - model_2_loss_2: 0.3293 - model_2_loss_3: 0.583 - ETA: 54s - loss: 19.0009 - model_2_loss_1: 18.0858 - model_2_loss_2: 0.3293 - model_2_loss_3: 0.585 - ETA: 54s - loss: 18.9624 - model_2_loss_1: 18.0459 - model_2_loss_2: 0.3301 - model_2_loss_3: 0.586 - ETA: 54s - loss: 18.9590 - model_2_loss_1: 18.0447 - model_2_loss_2: 0.3300 - model_2_loss_3: 0.584 - ETA: 53s - loss: 18.9113 - model_2_loss_1: 17.9988 - model_2_loss_2: 0.3297 - model_2_loss_3: 0.582 - ETA: 53s - loss: 18.8698 - model_2_loss_1: 17.9579 - model_2_loss_2: 0.3291 - model_2_loss_3: 0.582 - ETA: 52s - loss: 18.8424 - model_2_loss_1: 17.9314 - model_2_loss_2: 0.3291 - model_2_loss_3: 0.582 - ETA: 52s - loss: 18.8246 - model_2_loss_1: 17.9146 - model_2_loss_2: 0.3291 - model_2_loss_3: 0.580 - ETA: 52s - loss: 18.8682 - model_2_loss_1: 17.9595 - model_2_loss_2: 0.3289 - model_2_loss_3: 0.579 - ETA: 51s - loss: 18.8463 - model_2_loss_1: 17.9387 - model_2_loss_2: 0.3285 - model_2_loss_3: 0.579 - ETA: 51s - loss: 18.8046 - model_2_loss_1: 17.8978 - model_2_loss_2: 0.3281 - model_2_loss_3: 0.578 - ETA: 50s - loss: 18.7755 - model_2_loss_1: 17.8687 - model_2_loss_2: 0.3280 - model_2_loss_3: 0.578 - ETA: 50s - loss: 18.7563 - model_2_loss_1: 17.8490 - model_2_loss_2: 0.3275 - model_2_loss_3: 0.579 - ETA: 50s - loss: 18.7263 - model_2_loss_1: 17.8198 - model_2_loss_2: 0.3273 - model_2_loss_3: 0.579 - ETA: 49s - loss: 18.7074 - model_2_loss_1: 17.8005 - model_2_loss_2: 0.3270 - model_2_loss_3: 0.579 - ETA: 49s - loss: 18.6723 - model_2_loss_1: 17.7649 - model_2_loss_2: 0.3270 - model_2_loss_3: 0.580 - ETA: 48s - loss: 18.6433 - model_2_loss_1: 17.7366 - model_2_loss_2: 0.3267 - model_2_loss_3: 0.580 - ETA: 48s - loss: 18.6249 - model_2_loss_1: 17.7184 - model_2_loss_2: 0.3266 - model_2_loss_3: 0.579 - ETA: 48s - loss: 18.5918 - model_2_loss_1: 17.6863 - model_2_loss_2: 0.3266 - model_2_loss_3: 0.578 - ETA: 47s - loss: 18.5685 - model_2_loss_1: 17.6636 - model_2_loss_2: 0.3263 - model_2_loss_3: 0.578 - ETA: 47s - loss: 18.5486 - model_2_loss_1: 17.6443 - model_2_loss_2: 0.3260 - model_2_loss_3: 0.578 - ETA: 46s - loss: 18.5387 - model_2_loss_1: 17.6343 - model_2_loss_2: 0.3261 - model_2_loss_3: 0.578 - ETA: 46s - loss: 18.5031 - model_2_loss_1: 17.5990 - model_2_loss_2: 0.3258 - model_2_loss_3: 0.578 - ETA: 45s - loss: 18.4753 - model_2_loss_1: 17.5712 - model_2_loss_2: 0.3256 - model_2_loss_3: 0.578 - ETA: 45s - loss: 18.4523 - model_2_loss_1: 17.5488 - model_2_loss_2: 0.3255 - model_2_loss_3: 0.578 - ETA: 45s - loss: 18.4286 - model_2_loss_1: 17.5251 - model_2_loss_2: 0.3253 - model_2_loss_3: 0.578 - ETA: 44s - loss: 18.4048 - model_2_loss_1: 17.5012 - model_2_loss_2: 0.3253 - model_2_loss_3: 0.578 - ETA: 44s - loss: 18.3742 - model_2_loss_1: 17.4708 - model_2_loss_2: 0.3251 - model_2_loss_3: 0.578 - ETA: 43s - loss: 18.3764 - model_2_loss_1: 17.4737 - model_2_loss_2: 0.3249 - model_2_loss_3: 0.577 - ETA: 43s - loss: 18.3610 - model_2_loss_1: 17.4585 - model_2_loss_2: 0.3247 - model_2_loss_3: 0.577 - ETA: 43s - loss: 18.3318 - model_2_loss_1: 17.4294 - model_2_loss_2: 0.3244 - model_2_loss_3: 0.578 - ETA: 42s - loss: 18.3051 - model_2_loss_1: 17.4027 - model_2_loss_2: 0.3243 - model_2_loss_3: 0.578 - ETA: 42s - loss: 18.2778 - model_2_loss_1: 17.3758 - model_2_loss_2: 0.3241 - model_2_loss_3: 0.577 - ETA: 42s - loss: 18.2495 - model_2_loss_1: 17.3478 - model_2_loss_2: 0.3239 - model_2_loss_3: 0.577 - ETA: 42s - loss: 18.2257 - model_2_loss_1: 17.3242 - model_2_loss_2: 0.3237 - model_2_loss_3: 0.577 - ETA: 42s - loss: 18.2191 - model_2_loss_1: 17.3184 - model_2_loss_2: 0.3236 - model_2_loss_3: 0.577 - ETA: 41s - loss: 18.1906 - model_2_loss_1: 17.2900 - model_2_loss_2: 0.3235 - model_2_loss_3: 0.577 - ETA: 41s - loss: 18.1622 - model_2_loss_1: 17.2620 - model_2_loss_2: 0.3233 - model_2_loss_3: 0.576 - ETA: 41s - loss: 18.1356 - model_2_loss_1: 17.2355 - model_2_loss_2: 0.3231 - model_2_loss_3: 0.577 - ETA: 41s - loss: 18.1101 - model_2_loss_1: 17.2100 - model_2_loss_2: 0.3229 - model_2_loss_3: 0.577 - ETA: 40s - loss: 18.0830 - model_2_loss_1: 17.1833 - model_2_loss_2: 0.3228 - model_2_loss_3: 0.576 - ETA: 40s - loss: 18.0556 - model_2_loss_1: 17.1560 - model_2_loss_2: 0.3227 - model_2_loss_3: 0.576 - ETA: 40s - loss: 18.0283 - model_2_loss_1: 17.1292 - model_2_loss_2: 0.3225 - model_2_loss_3: 0.576 - ETA: 40s - loss: 18.0038 - model_2_loss_1: 17.1051 - model_2_loss_2: 0.3223 - model_2_loss_3: 0.576 - ETA: 39s - loss: 17.9760 - model_2_loss_1: 17.0773 - model_2_loss_2: 0.3222 - model_2_loss_3: 0.576 - ETA: 39s - loss: 17.9540 - model_2_loss_1: 17.0559 - model_2_loss_2: 0.3220 - model_2_loss_3: 0.576 - ETA: 39s - loss: 17.9338 - model_2_loss_1: 17.0359 - model_2_loss_2: 0.3217 - model_2_loss_3: 0.576 - ETA: 38s - loss: 17.9112 - model_2_loss_1: 17.0137 - model_2_loss_2: 0.3216 - model_2_loss_3: 0.575 - ETA: 38s - loss: 17.8864 - model_2_loss_1: 16.9886 - model_2_loss_2: 0.3215 - model_2_loss_3: 0.576 - ETA: 37s - loss: 17.8633 - model_2_loss_1: 16.9660 - model_2_loss_2: 0.3213 - model_2_loss_3: 0.575 - ETA: 37s - loss: 17.8409 - model_2_loss_1: 16.9439 - model_2_loss_2: 0.3211 - model_2_loss_3: 0.575 - ETA: 36s - loss: 17.8156 - model_2_loss_1: 16.9192 - model_2_loss_2: 0.3209 - model_2_loss_3: 0.575 - ETA: 36s - loss: 17.7896 - model_2_loss_1: 16.8937 - model_2_loss_2: 0.3207 - model_2_loss_3: 0.575 - ETA: 36s - loss: 17.7664 - model_2_loss_1: 16.8705 - model_2_loss_2: 0.3205 - model_2_loss_3: 0.575 - ETA: 35s - loss: 17.7439 - model_2_loss_1: 16.8483 - model_2_loss_2: 0.3203 - model_2_loss_3: 0.575 - ETA: 35s - loss: 17.7302 - model_2_loss_1: 16.8350 - model_2_loss_2: 0.3202 - model_2_loss_3: 0.574 - ETA: 34s - loss: 17.7079 - model_2_loss_1: 16.8130 - model_2_loss_2: 0.3200 - model_2_loss_3: 0.574 - ETA: 34s - loss: 17.6911 - model_2_loss_1: 16.7964 - model_2_loss_2: 0.3199 - model_2_loss_3: 0.574 - ETA: 33s - loss: 17.6692 - model_2_loss_1: 16.7749 - model_2_loss_2: 0.3197 - model_2_loss_3: 0.574 - ETA: 33s - loss: 17.6561 - model_2_loss_1: 16.7619 - model_2_loss_2: 0.3196 - model_2_loss_3: 0.574 - ETA: 32s - loss: 17.6367 - model_2_loss_1: 16.7431 - model_2_loss_2: 0.3194 - model_2_loss_3: 0.574 - ETA: 32s - loss: 17.6179 - model_2_loss_1: 16.7249 - model_2_loss_2: 0.3193 - model_2_loss_3: 0.573 - ETA: 31s - loss: 17.5954 - model_2_loss_1: 16.7025 - model_2_loss_2: 0.3193 - model_2_loss_3: 0.573 - ETA: 31s - loss: 17.5768 - model_2_loss_1: 16.6843 - model_2_loss_2: 0.3192 - model_2_loss_3: 0.573 - ETA: 30s - loss: 17.5541 - model_2_loss_1: 16.6618 - model_2_loss_2: 0.3190 - model_2_loss_3: 0.573 - ETA: 30s - loss: 17.5422 - model_2_loss_1: 16.6500 - model_2_loss_2: 0.3189 - model_2_loss_3: 0.573 - ETA: 30s - loss: 17.5183 - model_2_loss_1: 16.6265 - model_2_loss_2: 0.3187 - model_2_loss_3: 0.573 - ETA: 29s - loss: 17.4957 - model_2_loss_1: 16.6043 - model_2_loss_2: 0.3186 - model_2_loss_3: 0.572 - ETA: 29s - loss: 17.4802 - model_2_loss_1: 16.5892 - model_2_loss_2: 0.3185 - model_2_loss_3: 0.572 - ETA: 28s - loss: 17.4680 - model_2_loss_1: 16.5773 - model_2_loss_2: 0.3183 - model_2_loss_3: 0.572 - ETA: 28s - loss: 17.4489 - model_2_loss_1: 16.5587 - model_2_loss_2: 0.3182 - model_2_loss_3: 0.572 - ETA: 27s - loss: 17.4340 - model_2_loss_1: 16.5441 - model_2_loss_2: 0.3181 - model_2_loss_3: 0.571 - ETA: 27s - loss: 17.4122 - model_2_loss_1: 16.5228 - model_2_loss_2: 0.3179 - model_2_loss_3: 0.571 - ETA: 27s - loss: 17.3939 - model_2_loss_1: 16.5048 - model_2_loss_2: 0.3178 - model_2_loss_3: 0.571 - ETA: 26s - loss: 17.3741 - model_2_loss_1: 16.4853 - model_2_loss_2: 0.3177 - model_2_loss_3: 0.5710141/141 [==============================] - ETA: 26s - loss: 17.3553 - model_2_loss_1: 16.4669 - model_2_loss_2: 0.3176 - model_2_loss_3: 0.570 - ETA: 25s - loss: 17.3326 - model_2_loss_1: 16.4444 - model_2_loss_2: 0.3174 - model_2_loss_3: 0.570 - ETA: 25s - loss: 17.3127 - model_2_loss_1: 16.4251 - model_2_loss_2: 0.3172 - model_2_loss_3: 0.570 - ETA: 24s - loss: 17.3114 - model_2_loss_1: 16.4239 - model_2_loss_2: 0.3173 - model_2_loss_3: 0.570 - ETA: 24s - loss: 17.2959 - model_2_loss_1: 16.4085 - model_2_loss_2: 0.3171 - model_2_loss_3: 0.570 - ETA: 24s - loss: 17.2746 - model_2_loss_1: 16.3875 - model_2_loss_2: 0.3170 - model_2_loss_3: 0.570 - ETA: 23s - loss: 17.2532 - model_2_loss_1: 16.3663 - model_2_loss_2: 0.3168 - model_2_loss_3: 0.570 - ETA: 23s - loss: 17.2317 - model_2_loss_1: 16.3452 - model_2_loss_2: 0.3167 - model_2_loss_3: 0.569 - ETA: 22s - loss: 17.2103 - model_2_loss_1: 16.3244 - model_2_loss_2: 0.3165 - model_2_loss_3: 0.569 - ETA: 22s - loss: 17.1935 - model_2_loss_1: 16.3078 - model_2_loss_2: 0.3164 - model_2_loss_3: 0.569 - ETA: 21s - loss: 17.1705 - model_2_loss_1: 16.2851 - model_2_loss_2: 0.3163 - model_2_loss_3: 0.569 - ETA: 21s - loss: 17.1549 - model_2_loss_1: 16.2698 - model_2_loss_2: 0.3162 - model_2_loss_3: 0.568 - ETA: 21s - loss: 17.1333 - model_2_loss_1: 16.2487 - model_2_loss_2: 0.3161 - model_2_loss_3: 0.568 - ETA: 20s - loss: 17.1132 - model_2_loss_1: 16.2288 - model_2_loss_2: 0.3159 - model_2_loss_3: 0.568 - ETA: 20s - loss: 17.0968 - model_2_loss_1: 16.2126 - model_2_loss_2: 0.3158 - model_2_loss_3: 0.568 - ETA: 19s - loss: 17.0767 - model_2_loss_1: 16.1929 - model_2_loss_2: 0.3157 - model_2_loss_3: 0.568 - ETA: 19s - loss: 17.0565 - model_2_loss_1: 16.1731 - model_2_loss_2: 0.3155 - model_2_loss_3: 0.567 - ETA: 18s - loss: 17.0350 - model_2_loss_1: 16.1519 - model_2_loss_2: 0.3154 - model_2_loss_3: 0.567 - ETA: 18s - loss: 17.0118 - model_2_loss_1: 16.1288 - model_2_loss_2: 0.3153 - model_2_loss_3: 0.567 - ETA: 18s - loss: 16.9897 - model_2_loss_1: 16.1070 - model_2_loss_2: 0.3151 - model_2_loss_3: 0.567 - ETA: 17s - loss: 16.9674 - model_2_loss_1: 16.0851 - model_2_loss_2: 0.3150 - model_2_loss_3: 0.567 - ETA: 17s - loss: 16.9455 - model_2_loss_1: 16.0637 - model_2_loss_2: 0.3148 - model_2_loss_3: 0.566 - ETA: 16s - loss: 16.9280 - model_2_loss_1: 16.0467 - model_2_loss_2: 0.3147 - model_2_loss_3: 0.566 - ETA: 16s - loss: 16.9073 - model_2_loss_1: 16.0263 - model_2_loss_2: 0.3146 - model_2_loss_3: 0.566 - ETA: 15s - loss: 16.8844 - model_2_loss_1: 16.0037 - model_2_loss_2: 0.3144 - model_2_loss_3: 0.566 - ETA: 15s - loss: 16.8639 - model_2_loss_1: 15.9835 - model_2_loss_2: 0.3143 - model_2_loss_3: 0.566 - ETA: 15s - loss: 16.8436 - model_2_loss_1: 15.9636 - model_2_loss_2: 0.3142 - model_2_loss_3: 0.565 - ETA: 14s - loss: 16.8278 - model_2_loss_1: 15.9482 - model_2_loss_2: 0.3141 - model_2_loss_3: 0.565 - ETA: 14s - loss: 16.8080 - model_2_loss_1: 15.9288 - model_2_loss_2: 0.3140 - model_2_loss_3: 0.565 - ETA: 13s - loss: 16.7870 - model_2_loss_1: 15.9082 - model_2_loss_2: 0.3139 - model_2_loss_3: 0.564 - ETA: 13s - loss: 16.7650 - model_2_loss_1: 15.8867 - model_2_loss_2: 0.3137 - model_2_loss_3: 0.564 - ETA: 12s - loss: 16.7440 - model_2_loss_1: 15.8661 - model_2_loss_2: 0.3136 - model_2_loss_3: 0.564 - ETA: 12s - loss: 16.7251 - model_2_loss_1: 15.8475 - model_2_loss_2: 0.3135 - model_2_loss_3: 0.564 - ETA: 12s - loss: 16.7022 - model_2_loss_1: 15.8250 - model_2_loss_2: 0.3134 - model_2_loss_3: 0.563 - ETA: 11s - loss: 16.6861 - model_2_loss_1: 15.8092 - model_2_loss_2: 0.3133 - model_2_loss_3: 0.563 - ETA: 11s - loss: 16.6637 - model_2_loss_1: 15.7872 - model_2_loss_2: 0.3131 - model_2_loss_3: 0.563 - ETA: 10s - loss: 16.6428 - model_2_loss_1: 15.7665 - model_2_loss_2: 0.3130 - model_2_loss_3: 0.563 - ETA: 10s - loss: 16.6236 - model_2_loss_1: 15.7475 - model_2_loss_2: 0.3128 - model_2_loss_3: 0.563 - ETA: 9s - loss: 16.6032 - model_2_loss_1: 15.7274 - model_2_loss_2: 0.3127 - model_2_loss_3: 0.563 - ETA: 9s - loss: 16.5906 - model_2_loss_1: 15.7150 - model_2_loss_2: 0.3126 - model_2_loss_3: 0.56 - ETA: 8s - loss: 16.5690 - model_2_loss_1: 15.6938 - model_2_loss_2: 0.3125 - model_2_loss_3: 0.56 - ETA: 8s - loss: 16.5481 - model_2_loss_1: 15.6731 - model_2_loss_2: 0.3123 - model_2_loss_3: 0.56 - ETA: 8s - loss: 16.5282 - model_2_loss_1: 15.6537 - model_2_loss_2: 0.3121 - model_2_loss_3: 0.56 - ETA: 7s - loss: 16.5064 - model_2_loss_1: 15.6322 - model_2_loss_2: 0.3120 - model_2_loss_3: 0.56 - ETA: 7s - loss: 16.4865 - model_2_loss_1: 15.6126 - model_2_loss_2: 0.3118 - model_2_loss_3: 0.56 - ETA: 6s - loss: 16.4662 - model_2_loss_1: 15.5928 - model_2_loss_2: 0.3117 - model_2_loss_3: 0.56 - ETA: 6s - loss: 16.4443 - model_2_loss_1: 15.5713 - model_2_loss_2: 0.3115 - model_2_loss_3: 0.56 - ETA: 5s - loss: 16.4268 - model_2_loss_1: 15.5541 - model_2_loss_2: 0.3114 - model_2_loss_3: 0.56 - ETA: 5s - loss: 16.4072 - model_2_loss_1: 15.5349 - model_2_loss_2: 0.3113 - model_2_loss_3: 0.56 - ETA: 4s - loss: 16.3873 - model_2_loss_1: 15.5152 - model_2_loss_2: 0.3111 - model_2_loss_3: 0.56 - ETA: 4s - loss: 16.3682 - model_2_loss_1: 15.4963 - model_2_loss_2: 0.3110 - model_2_loss_3: 0.56 - ETA: 4s - loss: 16.3483 - model_2_loss_1: 15.4765 - model_2_loss_2: 0.3109 - model_2_loss_3: 0.56 - ETA: 3s - loss: 16.3269 - model_2_loss_1: 15.4556 - model_2_loss_2: 0.3107 - model_2_loss_3: 0.56 - ETA: 3s - loss: 16.3069 - model_2_loss_1: 15.4358 - model_2_loss_2: 0.3106 - model_2_loss_3: 0.56 - ETA: 2s - loss: 16.2893 - model_2_loss_1: 15.4185 - model_2_loss_2: 0.3105 - model_2_loss_3: 0.56 - ETA: 2s - loss: 16.2681 - model_2_loss_1: 15.3979 - model_2_loss_2: 0.3103 - model_2_loss_3: 0.55 - ETA: 1s - loss: 16.2470 - model_2_loss_1: 15.3770 - model_2_loss_2: 0.3102 - model_2_loss_3: 0.55 - ETA: 1s - loss: 16.2271 - model_2_loss_1: 15.3574 - model_2_loss_2: 0.3101 - model_2_loss_3: 0.55 - ETA: 0s - loss: 16.2058 - model_2_loss_1: 15.3364 - model_2_loss_2: 0.3100 - model_2_loss_3: 0.55 - ETA: 0s - loss: 16.1859 - model_2_loss_1: 15.3168 - model_2_loss_2: 0.3098 - model_2_loss_3: 0.55 - 64s - loss: 16.1631 - model_2_loss_1: 15.2942 - model_2_loss_2: 0.3097 - model_2_loss_3: 0.5592 - val_loss: 10.7176 - val_model_2_loss_1: 9.9077 - val_model_2_loss_2: 0.2743 - val_model_2_loss_3: 0.5356\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 80/141 [================>.............] - ETA: 65s - loss: 13.4346 - model_2_loss_1: 12.6181 - model_2_loss_2: 0.2887 - model_2_loss_3: 0.527 - ETA: 63s - loss: 13.2490 - model_2_loss_1: 12.4322 - model_2_loss_2: 0.2920 - model_2_loss_3: 0.524 - ETA: 63s - loss: 13.2086 - model_2_loss_1: 12.3915 - model_2_loss_2: 0.2926 - model_2_loss_3: 0.524 - ETA: 61s - loss: 13.1241 - model_2_loss_1: 12.3039 - model_2_loss_2: 0.2914 - model_2_loss_3: 0.528 - ETA: 61s - loss: 13.2508 - model_2_loss_1: 12.4306 - model_2_loss_2: 0.2918 - model_2_loss_3: 0.528 - ETA: 61s - loss: 13.2355 - model_2_loss_1: 12.4114 - model_2_loss_2: 0.2915 - model_2_loss_3: 0.532 - ETA: 61s - loss: 13.2117 - model_2_loss_1: 12.3870 - model_2_loss_2: 0.2917 - model_2_loss_3: 0.533 - ETA: 60s - loss: 13.1793 - model_2_loss_1: 12.3552 - model_2_loss_2: 0.2913 - model_2_loss_3: 0.532 - ETA: 60s - loss: 13.1372 - model_2_loss_1: 12.3133 - model_2_loss_2: 0.2908 - model_2_loss_3: 0.533 - ETA: 60s - loss: 13.1043 - model_2_loss_1: 12.2818 - model_2_loss_2: 0.2907 - model_2_loss_3: 0.531 - ETA: 59s - loss: 13.1268 - model_2_loss_1: 12.3066 - model_2_loss_2: 0.2905 - model_2_loss_3: 0.529 - ETA: 59s - loss: 13.0987 - model_2_loss_1: 12.2786 - model_2_loss_2: 0.2903 - model_2_loss_3: 0.529 - ETA: 58s - loss: 13.0582 - model_2_loss_1: 12.2380 - model_2_loss_2: 0.2903 - model_2_loss_3: 0.529 - ETA: 58s - loss: 13.0285 - model_2_loss_1: 12.2071 - model_2_loss_2: 0.2901 - model_2_loss_3: 0.531 - ETA: 57s - loss: 12.9874 - model_2_loss_1: 12.1671 - model_2_loss_2: 0.2899 - model_2_loss_3: 0.530 - ETA: 57s - loss: 13.0187 - model_2_loss_1: 12.1981 - model_2_loss_2: 0.2900 - model_2_loss_3: 0.530 - ETA: 57s - loss: 13.0042 - model_2_loss_1: 12.1834 - model_2_loss_2: 0.2901 - model_2_loss_3: 0.530 - ETA: 56s - loss: 12.9944 - model_2_loss_1: 12.1734 - model_2_loss_2: 0.2900 - model_2_loss_3: 0.531 - ETA: 56s - loss: 13.0116 - model_2_loss_1: 12.1903 - model_2_loss_2: 0.2899 - model_2_loss_3: 0.531 - ETA: 55s - loss: 12.9894 - model_2_loss_1: 12.1683 - model_2_loss_2: 0.2898 - model_2_loss_3: 0.531 - ETA: 54s - loss: 12.9883 - model_2_loss_1: 12.1673 - model_2_loss_2: 0.2900 - model_2_loss_3: 0.531 - ETA: 54s - loss: 12.9606 - model_2_loss_1: 12.1396 - model_2_loss_2: 0.2897 - model_2_loss_3: 0.531 - ETA: 54s - loss: 12.9292 - model_2_loss_1: 12.1081 - model_2_loss_2: 0.2894 - model_2_loss_3: 0.531 - ETA: 53s - loss: 12.9037 - model_2_loss_1: 12.0828 - model_2_loss_2: 0.2892 - model_2_loss_3: 0.531 - ETA: 53s - loss: 12.8777 - model_2_loss_1: 12.0573 - model_2_loss_2: 0.2891 - model_2_loss_3: 0.531 - ETA: 53s - loss: 12.8354 - model_2_loss_1: 12.0150 - model_2_loss_2: 0.2890 - model_2_loss_3: 0.531 - ETA: 52s - loss: 12.8318 - model_2_loss_1: 12.0120 - model_2_loss_2: 0.2888 - model_2_loss_3: 0.530 - ETA: 52s - loss: 12.8073 - model_2_loss_1: 11.9873 - model_2_loss_2: 0.2887 - model_2_loss_3: 0.531 - ETA: 51s - loss: 12.7907 - model_2_loss_1: 11.9710 - model_2_loss_2: 0.2885 - model_2_loss_3: 0.531 - ETA: 51s - loss: 12.7562 - model_2_loss_1: 11.9367 - model_2_loss_2: 0.2883 - model_2_loss_3: 0.531 - ETA: 50s - loss: 12.7223 - model_2_loss_1: 11.9025 - model_2_loss_2: 0.2884 - model_2_loss_3: 0.531 - ETA: 50s - loss: 12.7170 - model_2_loss_1: 11.8974 - model_2_loss_2: 0.2883 - model_2_loss_3: 0.531 - ETA: 49s - loss: 12.6924 - model_2_loss_1: 11.8726 - model_2_loss_2: 0.2881 - model_2_loss_3: 0.531 - ETA: 49s - loss: 12.6689 - model_2_loss_1: 11.8496 - model_2_loss_2: 0.2881 - model_2_loss_3: 0.531 - ETA: 48s - loss: 12.6541 - model_2_loss_1: 11.8352 - model_2_loss_2: 0.2881 - model_2_loss_3: 0.530 - ETA: 48s - loss: 12.6212 - model_2_loss_1: 11.8024 - model_2_loss_2: 0.2880 - model_2_loss_3: 0.530 - ETA: 47s - loss: 12.5991 - model_2_loss_1: 11.7806 - model_2_loss_2: 0.2879 - model_2_loss_3: 0.530 - ETA: 47s - loss: 12.5918 - model_2_loss_1: 11.7729 - model_2_loss_2: 0.2881 - model_2_loss_3: 0.530 - ETA: 46s - loss: 12.5613 - model_2_loss_1: 11.7423 - model_2_loss_2: 0.2880 - model_2_loss_3: 0.530 - ETA: 46s - loss: 12.5430 - model_2_loss_1: 11.7242 - model_2_loss_2: 0.2880 - model_2_loss_3: 0.530 - ETA: 46s - loss: 12.5117 - model_2_loss_1: 11.6930 - model_2_loss_2: 0.2879 - model_2_loss_3: 0.530 - ETA: 45s - loss: 12.4810 - model_2_loss_1: 11.6625 - model_2_loss_2: 0.2878 - model_2_loss_3: 0.530 - ETA: 45s - loss: 12.4497 - model_2_loss_1: 11.6312 - model_2_loss_2: 0.2877 - model_2_loss_3: 0.530 - ETA: 44s - loss: 12.4246 - model_2_loss_1: 11.6063 - model_2_loss_2: 0.2876 - model_2_loss_3: 0.530 - ETA: 44s - loss: 12.3940 - model_2_loss_1: 11.5760 - model_2_loss_2: 0.2876 - model_2_loss_3: 0.530 - ETA: 43s - loss: 12.3597 - model_2_loss_1: 11.5419 - model_2_loss_2: 0.2875 - model_2_loss_3: 0.530 - ETA: 43s - loss: 12.3270 - model_2_loss_1: 11.5091 - model_2_loss_2: 0.2875 - model_2_loss_3: 0.530 - ETA: 42s - loss: 12.3025 - model_2_loss_1: 11.4850 - model_2_loss_2: 0.2875 - model_2_loss_3: 0.530 - ETA: 42s - loss: 12.2682 - model_2_loss_1: 11.4508 - model_2_loss_2: 0.2874 - model_2_loss_3: 0.529 - ETA: 41s - loss: 12.2385 - model_2_loss_1: 11.4212 - model_2_loss_2: 0.2874 - model_2_loss_3: 0.530 - ETA: 41s - loss: 12.2097 - model_2_loss_1: 11.3924 - model_2_loss_2: 0.2874 - model_2_loss_3: 0.529 - ETA: 40s - loss: 12.1834 - model_2_loss_1: 11.3661 - model_2_loss_2: 0.2874 - model_2_loss_3: 0.529 - ETA: 40s - loss: 12.1560 - model_2_loss_1: 11.3388 - model_2_loss_2: 0.2874 - model_2_loss_3: 0.529 - ETA: 39s - loss: 12.1445 - model_2_loss_1: 11.3271 - model_2_loss_2: 0.2874 - model_2_loss_3: 0.530 - ETA: 39s - loss: 12.1146 - model_2_loss_1: 11.2971 - model_2_loss_2: 0.2874 - model_2_loss_3: 0.530 - ETA: 39s - loss: 12.0832 - model_2_loss_1: 11.2657 - model_2_loss_2: 0.2873 - model_2_loss_3: 0.530 - ETA: 38s - loss: 12.0521 - model_2_loss_1: 11.2350 - model_2_loss_2: 0.2873 - model_2_loss_3: 0.529 - ETA: 38s - loss: 12.0213 - model_2_loss_1: 11.2044 - model_2_loss_2: 0.2873 - model_2_loss_3: 0.529 - ETA: 38s - loss: 11.9896 - model_2_loss_1: 11.1728 - model_2_loss_2: 0.2873 - model_2_loss_3: 0.529 - ETA: 37s - loss: 11.9591 - model_2_loss_1: 11.1426 - model_2_loss_2: 0.2873 - model_2_loss_3: 0.529 - ETA: 37s - loss: 11.9257 - model_2_loss_1: 11.1091 - model_2_loss_2: 0.2872 - model_2_loss_3: 0.529 - ETA: 37s - loss: 11.8950 - model_2_loss_1: 11.0782 - model_2_loss_2: 0.2873 - model_2_loss_3: 0.529 - ETA: 37s - loss: 11.8609 - model_2_loss_1: 11.0442 - model_2_loss_2: 0.2872 - model_2_loss_3: 0.529 - ETA: 36s - loss: 11.8232 - model_2_loss_1: 11.0067 - model_2_loss_2: 0.2872 - model_2_loss_3: 0.529 - ETA: 36s - loss: 11.7874 - model_2_loss_1: 10.9711 - model_2_loss_2: 0.2871 - model_2_loss_3: 0.529 - ETA: 35s - loss: 11.7568 - model_2_loss_1: 10.9408 - model_2_loss_2: 0.2871 - model_2_loss_3: 0.528 - ETA: 35s - loss: 11.7342 - model_2_loss_1: 10.9184 - model_2_loss_2: 0.2871 - model_2_loss_3: 0.528 - ETA: 34s - loss: 11.7081 - model_2_loss_1: 10.8924 - model_2_loss_2: 0.2871 - model_2_loss_3: 0.528 - ETA: 34s - loss: 11.6759 - model_2_loss_1: 10.8603 - model_2_loss_2: 0.2870 - model_2_loss_3: 0.528 - ETA: 33s - loss: 11.6530 - model_2_loss_1: 10.8376 - model_2_loss_2: 0.2870 - model_2_loss_3: 0.528 - ETA: 33s - loss: 11.6199 - model_2_loss_1: 10.8047 - model_2_loss_2: 0.2870 - model_2_loss_3: 0.528 - ETA: 33s - loss: 11.5823 - model_2_loss_1: 10.7671 - model_2_loss_2: 0.2869 - model_2_loss_3: 0.528 - ETA: 32s - loss: 11.5427 - model_2_loss_1: 10.7276 - model_2_loss_2: 0.2869 - model_2_loss_3: 0.528 - ETA: 32s - loss: 11.5095 - model_2_loss_1: 10.6945 - model_2_loss_2: 0.2869 - model_2_loss_3: 0.528 - ETA: 31s - loss: 11.4687 - model_2_loss_1: 10.6537 - model_2_loss_2: 0.2869 - model_2_loss_3: 0.528 - ETA: 31s - loss: 11.4328 - model_2_loss_1: 10.6181 - model_2_loss_2: 0.2869 - model_2_loss_3: 0.527 - ETA: 30s - loss: 11.3966 - model_2_loss_1: 10.5820 - model_2_loss_2: 0.2869 - model_2_loss_3: 0.527 - ETA: 30s - loss: 11.3642 - model_2_loss_1: 10.5497 - model_2_loss_2: 0.2869 - model_2_loss_3: 0.527 - ETA: 29s - loss: 11.3283 - model_2_loss_1: 10.5137 - model_2_loss_2: 0.2869 - model_2_loss_3: 0.527 - ETA: 29s - loss: 11.2990 - model_2_loss_1: 10.4845 - model_2_loss_2: 0.2869 - model_2_loss_3: 0.5276"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140/141 [============================>.] - ETA: 28s - loss: 11.2623 - model_2_loss_1: 10.4478 - model_2_loss_2: 0.2869 - model_2_loss_3: 0.527 - ETA: 28s - loss: 11.2252 - model_2_loss_1: 10.4109 - model_2_loss_2: 0.2869 - model_2_loss_3: 0.527 - ETA: 27s - loss: 11.1887 - model_2_loss_1: 10.3744 - model_2_loss_2: 0.2869 - model_2_loss_3: 0.527 - ETA: 27s - loss: 11.1502 - model_2_loss_1: 10.3359 - model_2_loss_2: 0.2869 - model_2_loss_3: 0.527 - ETA: 27s - loss: 11.1125 - model_2_loss_1: 10.2982 - model_2_loss_2: 0.2869 - model_2_loss_3: 0.527 - ETA: 26s - loss: 11.0841 - model_2_loss_1: 10.2700 - model_2_loss_2: 0.2869 - model_2_loss_3: 0.527 - ETA: 26s - loss: 11.0488 - model_2_loss_1: 10.2346 - model_2_loss_2: 0.2869 - model_2_loss_3: 0.527 - ETA: 25s - loss: 11.0090 - model_2_loss_1: 10.1950 - model_2_loss_2: 0.2869 - model_2_loss_3: 0.527 - ETA: 25s - loss: 10.9664 - model_2_loss_1: 10.1524 - model_2_loss_2: 0.2869 - model_2_loss_3: 0.527 - ETA: 24s - loss: 10.9277 - model_2_loss_1: 10.1138 - model_2_loss_2: 0.2869 - model_2_loss_3: 0.527 - ETA: 24s - loss: 10.8868 - model_2_loss_1: 10.0727 - model_2_loss_2: 0.2869 - model_2_loss_3: 0.527 - ETA: 23s - loss: 10.8452 - model_2_loss_1: 10.0313 - model_2_loss_2: 0.2869 - model_2_loss_3: 0.527 - ETA: 23s - loss: 10.8005 - model_2_loss_1: 9.9867 - model_2_loss_2: 0.2869 - model_2_loss_3: 0.526 - ETA: 22s - loss: 10.7585 - model_2_loss_1: 9.9447 - model_2_loss_2: 0.2869 - model_2_loss_3: 0.52 - ETA: 22s - loss: 10.7310 - model_2_loss_1: 9.9173 - model_2_loss_2: 0.2869 - model_2_loss_3: 0.52 - ETA: 21s - loss: 10.6918 - model_2_loss_1: 9.8782 - model_2_loss_2: 0.2869 - model_2_loss_3: 0.52 - ETA: 21s - loss: 10.6492 - model_2_loss_1: 9.8357 - model_2_loss_2: 0.2869 - model_2_loss_3: 0.52 - ETA: 20s - loss: 10.6140 - model_2_loss_1: 9.8006 - model_2_loss_2: 0.2869 - model_2_loss_3: 0.52 - ETA: 20s - loss: 10.5751 - model_2_loss_1: 9.7618 - model_2_loss_2: 0.2869 - model_2_loss_3: 0.52 - ETA: 19s - loss: 10.5370 - model_2_loss_1: 9.7239 - model_2_loss_2: 0.2869 - model_2_loss_3: 0.52 - ETA: 19s - loss: 10.4974 - model_2_loss_1: 9.6842 - model_2_loss_2: 0.2869 - model_2_loss_3: 0.52 - ETA: 18s - loss: 10.4573 - model_2_loss_1: 9.6441 - model_2_loss_2: 0.2869 - model_2_loss_3: 0.52 - ETA: 18s - loss: 10.4177 - model_2_loss_1: 9.6047 - model_2_loss_2: 0.2869 - model_2_loss_3: 0.52 - ETA: 17s - loss: 10.3761 - model_2_loss_1: 9.5630 - model_2_loss_2: 0.2870 - model_2_loss_3: 0.52 - ETA: 17s - loss: 10.3386 - model_2_loss_1: 9.5254 - model_2_loss_2: 0.2870 - model_2_loss_3: 0.52 - ETA: 16s - loss: 10.3018 - model_2_loss_1: 9.4887 - model_2_loss_2: 0.2870 - model_2_loss_3: 0.52 - ETA: 16s - loss: 10.2627 - model_2_loss_1: 9.4497 - model_2_loss_2: 0.2870 - model_2_loss_3: 0.52 - ETA: 16s - loss: 10.2223 - model_2_loss_1: 9.4094 - model_2_loss_2: 0.2870 - model_2_loss_3: 0.52 - ETA: 15s - loss: 10.1827 - model_2_loss_1: 9.3697 - model_2_loss_2: 0.2870 - model_2_loss_3: 0.52 - ETA: 15s - loss: 10.1436 - model_2_loss_1: 9.3307 - model_2_loss_2: 0.2870 - model_2_loss_3: 0.52 - ETA: 14s - loss: 10.1055 - model_2_loss_1: 9.2928 - model_2_loss_2: 0.2870 - model_2_loss_3: 0.52 - ETA: 14s - loss: 10.0702 - model_2_loss_1: 9.2576 - model_2_loss_2: 0.2870 - model_2_loss_3: 0.52 - ETA: 13s - loss: 10.0506 - model_2_loss_1: 9.2381 - model_2_loss_2: 0.2870 - model_2_loss_3: 0.52 - ETA: 13s - loss: 10.0210 - model_2_loss_1: 9.2084 - model_2_loss_2: 0.2870 - model_2_loss_3: 0.52 - ETA: 12s - loss: 9.9825 - model_2_loss_1: 9.1700 - model_2_loss_2: 0.2869 - model_2_loss_3: 0.5255 - ETA: 12s - loss: 9.9449 - model_2_loss_1: 9.1326 - model_2_loss_2: 0.2869 - model_2_loss_3: 0.525 - ETA: 11s - loss: 9.9107 - model_2_loss_1: 9.0986 - model_2_loss_2: 0.2869 - model_2_loss_3: 0.525 - ETA: 11s - loss: 9.8758 - model_2_loss_1: 9.0638 - model_2_loss_2: 0.2869 - model_2_loss_3: 0.525 - ETA: 10s - loss: 9.8380 - model_2_loss_1: 9.0260 - model_2_loss_2: 0.2869 - model_2_loss_3: 0.525 - ETA: 10s - loss: 9.8038 - model_2_loss_1: 8.9920 - model_2_loss_2: 0.2868 - model_2_loss_3: 0.525 - ETA: 9s - loss: 9.7680 - model_2_loss_1: 8.9563 - model_2_loss_2: 0.2868 - model_2_loss_3: 0.524 - ETA: 9s - loss: 9.7324 - model_2_loss_1: 8.9209 - model_2_loss_2: 0.2868 - model_2_loss_3: 0.52 - ETA: 8s - loss: 9.6968 - model_2_loss_1: 8.8854 - model_2_loss_2: 0.2867 - model_2_loss_3: 0.52 - ETA: 8s - loss: 9.6627 - model_2_loss_1: 8.8514 - model_2_loss_2: 0.2867 - model_2_loss_3: 0.52 - ETA: 7s - loss: 9.6390 - model_2_loss_1: 8.8278 - model_2_loss_2: 0.2867 - model_2_loss_3: 0.52 - ETA: 7s - loss: 9.6044 - model_2_loss_1: 8.7933 - model_2_loss_2: 0.2867 - model_2_loss_3: 0.52 - ETA: 6s - loss: 9.5724 - model_2_loss_1: 8.7615 - model_2_loss_2: 0.2866 - model_2_loss_3: 0.52 - ETA: 6s - loss: 9.5430 - model_2_loss_1: 8.7323 - model_2_loss_2: 0.2866 - model_2_loss_3: 0.52 - ETA: 5s - loss: 9.5099 - model_2_loss_1: 8.6992 - model_2_loss_2: 0.2866 - model_2_loss_3: 0.52 - ETA: 5s - loss: 9.4781 - model_2_loss_1: 8.6676 - model_2_loss_2: 0.2866 - model_2_loss_3: 0.52 - ETA: 4s - loss: 9.4541 - model_2_loss_1: 8.6436 - model_2_loss_2: 0.2866 - model_2_loss_3: 0.52 - ETA: 4s - loss: 9.4219 - model_2_loss_1: 8.6116 - model_2_loss_2: 0.2866 - model_2_loss_3: 0.52 - ETA: 3s - loss: 9.3908 - model_2_loss_1: 8.5806 - model_2_loss_2: 0.2865 - model_2_loss_3: 0.52 - ETA: 3s - loss: 9.3614 - model_2_loss_1: 8.5513 - model_2_loss_2: 0.2865 - model_2_loss_3: 0.52 - ETA: 2s - loss: 9.3353 - model_2_loss_1: 8.5253 - model_2_loss_2: 0.2865 - model_2_loss_3: 0.52 - ETA: 2s - loss: 9.3056 - model_2_loss_1: 8.4957 - model_2_loss_2: 0.2865 - model_2_loss_3: 0.52 - ETA: 1s - loss: 9.2735 - model_2_loss_1: 8.4637 - model_2_loss_2: 0.2864 - model_2_loss_3: 0.52 - ETA: 1s - loss: 9.2453 - model_2_loss_1: 8.4357 - model_2_loss_2: 0.2864 - model_2_loss_3: 0.52 - ETA: 0s - loss: 9.2146 - model_2_loss_1: 8.4052 - model_2_loss_2: 0.2864 - model_2_loss_3: 0.52 - ETA: 0s - loss: 9.1874 - model_2_loss_1: 8.3781 - model_2_loss_2: 0.2863 - model_2_loss_3: 0.5230"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "last_test = ((test_idx.shape[0] // batch_size)*batch_size) - test_idx.shape[0]\n",
    "last_test = test_idx.shape[0] if last_test == 0 else last_test\n",
    "last_train = ((train_idx.shape[0] // batch_size)*batch_size) - train_idx.shape[0]\n",
    "last_train = train_idx.shape[0] if last_train == 0 else last_train\n",
    "\n",
    "h = VAE.fit_generator(\n",
    "    data_generator(data, last_train, train_idx, scaler, log_scaler, action_scaler, batch_size=batch_size, verbose=False),\n",
    "    train_idx[:last_train].shape[0]/batch_size, \n",
    "    verbose=1,\n",
    "    epochs=epochs,\n",
    "    validation_data=data_generator(data, last_test, test_idx, scaler, log_scaler, action_scaler, batch_size=batch_size),\n",
    "    validation_steps=test_idx[:last_test].shape[0]/batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_action_bin, X_features_bin, X_num, X_action, X_finished, X_alive = matrify(data, test_idx, scaler, log_scaler,action_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_action_bin, X_features_bin, X_num, X_action, X_finished, X_alive = X_action_bin[:last_test], X_features_bin[:last_test], X_num[:last_test], X_action[:last_test], X_finished[:last_test], X_alive[:last_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`vitals, binary_features, binary_actions, actions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_recons, finished, dead_or_alive = VAE.predict([X_num, X_features_bin, X_action_bin, X_action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_num[0][15:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for feature, error in zip(numerical_columns_not_to_be_logged+numerical_columns_to_be_logged, np.mean((X_recons[0] - X_num[0])[:16]**2, 0)):\n",
    "    print(feature, error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.sqrt(0.23)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
