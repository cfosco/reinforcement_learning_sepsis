{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MountainCar Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import LeakyReLU, ReLU, SELU\n",
    "import numpy as np\n",
    "from random import random, sample\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from IPython.display import display, clear_output\n",
    "from parse_dataset import *\n",
    "import icnn\n",
    "from icnn import ICNN, ICNNBN, gradient_step_action, diff_params, clip_gradients, get_q_target, update_parameters_lag\n",
    "from copy import deepcopy\n",
    "from utils import variable, moving_avg\n",
    "from time import time\n",
    "import evaluation\n",
    "import importlib\n",
    "import gym\n",
    "import math\n",
    "from moving_particle import get_q_target, argmax, clip, create_dataset\n",
    "from replay_buffer import PrioritizedReplayBuffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic gym tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.58729765,  0.        ])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "transitions = []\n",
    "episodes = 100\n",
    "iterations = 100\n",
    "\n",
    "for e in range(episodes):\n",
    "    s = env.reset()\n",
    "    for i in range(iterations):\n",
    "        action = np.random.uniform(-1,1,(1,)) #env.action_space.sample()\n",
    "        former = env.env.state*1\n",
    "        s_, reward, done, info = env.step(action)\n",
    "        transitions.append((s, action, reward, s_, done))\n",
    "        s = s_*1\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(1,)\n",
      "[ 1.] [-1.]\n",
      "[ 0.6   0.07] [-1.2  -0.07]\n",
      "Box(2,)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)\n",
    "print(env.action_space.high,env.action_space.low)\n",
    "print(env.observation_space.high,env.observation_space.low)\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting PER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# memory = PrioritizedReplayBuffer(100000, .6)\n",
    "memory = PrioritizedReplayBuffer(100000, .6)\n",
    "for s,a,r,s_,done in transitions:\n",
    "    memory.add(*(s,a,r,s_,np.array([done])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "super(type, obj): obj must be an instance or subtype of type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-ecf28f9c7e8b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mqs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mICNNBN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m1.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLeakyReLU\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mqe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mqe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'#params: %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mqs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sebastien\\anaconda_notebook\\cs282-f17-sebastian-camilo\\icnn.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, n_layers, hidden_dim, input_dim, action_dim, gain, RMAX, activation)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m1.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRMAX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLeakyReLU\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mICNNBN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_layers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRMAX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRMAX\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: super(type, obj): obj must be an instance or subtype of type"
     ]
    }
   ],
   "source": [
    "qs = ICNNBN(3, 20, 2, action_dim = 1, gain=math.sqrt(2/1.01), activation=LeakyReLU(.01))\n",
    "qe = deepcopy(qs)\n",
    "qe.eval()\n",
    "print('#params: %s' % np.sum([np.prod(p.data.numpy().shape) for p in qs.parameters()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "optimizer = t.optim.Adam(qs.parameters(), lr=learning_rate, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# monitoring\n",
    "losses = []\n",
    "td_errors = []\n",
    "test_td_errors = []\n",
    "test_average_q_pred = []\n",
    "test_max_q_pred = []\n",
    "test_min_q_pred = []\n",
    "average_q_pred = []\n",
    "average_q_target = []\n",
    "max_q_pred = []\n",
    "min_q_pred = []\n",
    "max_q_target = []\n",
    "min_q_target = []\n",
    "average_a = []\n",
    "min_a = []\n",
    "max_a = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that some functions are redefined in the cell that follows the next one**\n",
    "\n",
    "Run it BEFORE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (9,9)\n",
    "batch_size = 256\n",
    "global_step = -1\n",
    "tau = 1e-3\n",
    "c = 1e5\n",
    "max_steps_a = 10\n",
    "gamma = .99\n",
    "max_steps_beta = 100000\n",
    "beta0 = .9\n",
    "beta = lambda t: (t<max_steps_beta)*(beta0 + t*(1-beta0)/max_steps_beta) + (t>=max_steps_beta)*1.\n",
    "all_rewards = [0]\n",
    "\n",
    "RMIN, RMAX = -1, 100\n",
    "\n",
    "ss = env.reset()\n",
    "\n",
    "while True:\n",
    "    global_step += 1\n",
    "    \n",
    "    # act\n",
    "    a = argmax(qs, variable(ss.reshape((1,-1)))).data.numpy()[0]\n",
    "    ss_, reward, done, info = env.step(a)\n",
    "    all_rewards.append(all_rewards[-1]+reward)\n",
    "    memory.add(*(ss,a,reward,ss_,np.array([done])))\n",
    "    ss = ss_\n",
    "    if done:\n",
    "        ss = env.reset()\n",
    "    \n",
    "    # sample\n",
    "    s,a,r,s_,done,w,idx = memory.sample(batch_size, beta(global_step))\n",
    "    states = variable(s)\n",
    "    actions = variable(a.astype(np.float32))\n",
    "    rewards = variable(r)\n",
    "    next_states = variable(s_)\n",
    "    weights = variable(w).squeeze()\n",
    "    \n",
    "    # init grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Compute loss\n",
    "    pred = qs.forward(states,actions).squeeze()  # Q(s, a)\n",
    "\n",
    "    target, max_actions = get_q_target(qs, qe, next_states, rewards, batch_size, done, min0=-1, max0=1, gamma=gamma, max_steps_a=max_steps_a) # r + g * max_a' Q(s', a')\n",
    "    target = t.max(variable(pred.data) - 1., target)  # clip targets (il font comme ca dans le code icnn)\n",
    "    target = t.min(variable(pred.data) + 1., target)\n",
    "    assert not target.requires_grad\n",
    "\n",
    "    loss = huber_loss(pred, target)  # j'ai vu quelques personnes utiliser ca\n",
    "    loss = loss*weights  # multiplication by the PER coefficients\n",
    "    loss = t.mean(loss) + c*t.mean(loss_beyond_RMAX(pred, RMIN, RMAX))\n",
    "\n",
    "    # Update priorities\n",
    "    td_error = t.abs(pred-target)+1e-2\n",
    "    memory.update_priorities(idx, td_error.data.numpy())\n",
    "    \n",
    "    # Monitoring\n",
    "    losses.append(loss.data.numpy()[0])\n",
    "    td_errors.append(td_error.data.numpy().mean())\n",
    "    average_q_pred.append(t.mean(pred).squeeze().data.numpy()[0])\n",
    "    max_q_pred.append(t.max(pred).squeeze().data.numpy()[0])\n",
    "    min_q_pred.append(t.min(pred).squeeze().data.numpy()[0])\n",
    "    average_q_target.append(t.mean(target).squeeze().data.numpy()[0])\n",
    "    max_q_target.append(t.max(target).squeeze().data.numpy()[0])\n",
    "    min_q_target.append(t.min(target).squeeze().data.numpy()[0])\n",
    "    average_a.append(max_actions.mean(0)[0])\n",
    "    max_a.append(max_actions.max(0)[0])\n",
    "    min_a.append(max_actions.min(0)[0])\n",
    "    \n",
    "    # Compute gradients and update weights of the selection network\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # update parameters of the evaluation network. Its weights lag behind\n",
    "    update_parameters_lag(qs, qe, tau)\n",
    "    \n",
    "    # PLOT\n",
    "    if global_step % 250 == 0:\n",
    "        clear_output(wait=True)\n",
    "        try:\n",
    "            print('250 steps seen in %s' % str(time()-t0))\n",
    "        except:\n",
    "            t0 = time()\n",
    "        t0 = time()\n",
    "\n",
    "        fig, axes = plt.subplots(3, 2)\n",
    "        # td error\n",
    "        axes[0,0].plot(moving_avg(np.log(losses)))\n",
    "        axes[0,1].plot(moving_avg(td_errors))\n",
    "        # Q values (target and pred)\n",
    "        axes[1,0].plot(moving_avg(average_q_pred), label='avg pred', c='darkblue')\n",
    "        axes[1,0].plot(moving_avg(min_q_pred), label='min pred', c='darkblue', alpha=.8,linestyle=':')\n",
    "        axes[1,0].plot(moving_avg(max_q_pred), label='max pred', c='darkblue', alpha=.8,linestyle=':')\n",
    "        axes[1,0].plot(moving_avg(average_q_target), label='avg target', c='crimson')\n",
    "        axes[1,0].plot(moving_avg(min_q_target), label='min target', c='crimson', alpha=.8,linestyle=':')\n",
    "        axes[1,0].plot(moving_avg(max_q_target), label='max target', c='crimson', alpha=.8,linestyle=':')\n",
    "        axes[1,1].plot(moving_avg(average_a), c='r')\n",
    "        axes[1,1].plot(moving_avg(min_a), c='r', linestyle=':',alpha=.8)\n",
    "        axes[1,1].plot(moving_avg(max_a), c='r', linestyle=':',alpha=.8)\n",
    "        axes[2,0].plot(all_rewards)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# some utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch as t\n",
    "def loss_beyond_RMAX(x, RMIN, RMAX):\n",
    "    return (x > RMAX).float()*(x - RMAX)**2 + (x < RMIN).float()*(x - RMIN)**2\n",
    "\n",
    "def huber_loss(x,y):\n",
    "    d = x-y\n",
    "    return .5*(d**2)*(t.abs(d)<1).float() + (t.abs(d) - .5)*(t.abs(d)>=1).float()\n",
    "\n",
    "def loss_range(a, min0, max0, c):\n",
    "    return c*(\n",
    "    (a > max0).float()*t.abs(a[:] - max0) +\n",
    "    (a < min0).float()*t.abs(a[:] - min0) \n",
    "    )\n",
    "\n",
    "def gradient_step_action(Q, s, a, min0, max0, c=1e5, input_param=None, optimizer=None):\n",
    "    if input_param is None or optimizer is None:\n",
    "        input_param = t.nn.Parameter(a.data)\n",
    "        optimizer = t.optim.Rprop([input_param], lr=5e-1)\n",
    "\n",
    "    assert len(s) == len(a), 'There should be as many states as there are actions'\n",
    "    batch_size = len(s)\n",
    "\n",
    "    # erase previous gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # trick to get the gradients wrt `a`\n",
    "    grad = {}\n",
    "    def f(x):\n",
    "        grad['a'] = x\n",
    "    a.register_hook(f)\n",
    "\n",
    "    # get output (we want to maximize Q, so minimize -Q (the optimizer minimizes by default))\n",
    "    output = -Q(s, a) + loss_range(a, min0, max0, c)\n",
    "\n",
    "    # compute gradients\n",
    "    output.backward(t.FloatTensor(batch_size*[[1.]]))\n",
    "\n",
    "    # use the gradients that was deceitfully obtained using the hook\n",
    "    input_param.grad = grad['a']\n",
    "\n",
    "    # update the action\n",
    "    optimizer.step()\n",
    "\n",
    "    # returns the new value of `a` (a pytorch variable), the same thing but wrapped in t.nn.Parameter, and the optimizer\n",
    "    return variable(input_param.data, requires_grad=True), input_param, optimizer\n",
    "\n",
    "\n",
    "def get_q_target(Q_select, Q_eval, next_states, rewards, batch_size, done, min0=-1, max0=1, gamma=1., max_steps_a=10):\n",
    "    Q_target = rewards.squeeze().data.numpy() * 1\n",
    "    action_dim = Q_select.action_dim\n",
    "    Q_select.eval()\n",
    "    Q_eval.eval()\n",
    "\n",
    "    # identify non-terminal states\n",
    "    mask = t.from_numpy((1 - done)).byte().view(-1, 1)\n",
    "    shape = int(next_states.data.numpy().shape[1])\n",
    "    masked_next_states = next_states.masked_select(mask).resize(int((1 - done).sum()), shape)\n",
    "\n",
    "    # maximize\n",
    "    max_action = variable(np.zeros((len(masked_next_states), action_dim)), requires_grad=True).float()\n",
    "    prev_action = variable(np.zeros((len(masked_next_states), action_dim)), requires_grad=True).float()\n",
    "    input_param = t.nn.Parameter(max_action.data)\n",
    "    optimizer_for_a = t.optim.Rprop([input_param], lr=5e-1)\n",
    "    for k in range(max_steps_a):\n",
    "        max_action, input_param, optimizer_for_a = gradient_step_action(Q_select, masked_next_states, max_action, min0, max0, input_param=input_param, c=1e6, optimizer=optimizer_for_a)\n",
    "        if np.max(np.abs(prev_action.data.numpy() - max_action.data.numpy())) < 1e-3:\n",
    "            break\n",
    "        prev_action = max_action * 1\n",
    "\n",
    "    pred = Q_eval.forward(masked_next_states, max_action).squeeze()\n",
    "\n",
    "    np_mask = mask.squeeze().numpy()\n",
    "    np_mask = [k for k in range(batch_size) if np_mask[k] == 1]\n",
    "    Q_target[np_mask] += gamma * (pred.data.numpy())\n",
    "\n",
    "    Q_select.train()\n",
    "\n",
    "    return variable(Q_target), max_action.data.numpy()\n",
    "\n",
    "def argmax(Q_select, s, max_steps_a=10, min0=-1, max0=1):\n",
    "    Q_select.eval()\n",
    "    # maximize\n",
    "    max_action = variable(np.zeros((len(s), 1)), requires_grad=True).float()\n",
    "    prev_action = variable(np.zeros((len(s), 1)), requires_grad=True).float()\n",
    "    input_param = t.nn.Parameter(max_action.data)\n",
    "    optimizer_for_a = t.optim.Rprop([input_param], lr=5e-1)\n",
    "    for k in range(max_steps_a):\n",
    "        max_action, input_param, optimizer_for_a = gradient_step_action(Q_select, s, max_action, min0, max0, input_param=input_param, optimizer=optimizer_for_a)\n",
    "        if np.max(np.abs(prev_action.data.numpy() - max_action.data.numpy())) < 1e-3:\n",
    "            break\n",
    "        prev_action = max_action * 1\n",
    "    return max_action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
