{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import torch as t\n",
    "from torch.nn import ReLU, SELU, LeakyReLU\n",
    "import numpy as np\n",
    "from random import random, sample\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from parse_dataset import *\n",
    "from icnn import ICNN, ICNNBN, gradient_step_action, diff_params, loss_beyond_RMAX, clip_gradients, get_q_target, update_parameters_lag\n",
    "from copy import deepcopy\n",
    "from utils import variable, moving_avg\n",
    "from replay_buffer import PrioritizedReplayBuffer\n",
    "from IPython.display import display, clear_output\n",
    "from time import time\n",
    "import evaluation\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and parse Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('Sepsis_imp.csv')\n",
    "replace_absurd_temperatures(data)\n",
    "data = drop_patients_with_absurd_weights(data)\n",
    "data = drop_patients_with_unrealistic_HR_or_BP(data)\n",
    "data = add_relative_time_column(data)\n",
    "data = drop_patient_with_negative_input(data)\n",
    "add_small_quantities(data)\n",
    "create_action_column(data)\n",
    "add_log_actions(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build transitions array\n",
    "# limit_patients = True\n",
    "log_scaler = StandardScaler()\n",
    "scaler = StandardScaler()\n",
    "action_scaler = StandardScaler()\n",
    "train_idx, test_idx = split_train_test_idx(data)\n",
    "\n",
    "# scale on train data only\n",
    "scaler.fit(data.loc[data.icustayid.isin(train_idx)][numerical_columns_not_to_be_logged])\n",
    "log_scaler.fit(np.log(data.loc[data.icustayid.isin(train_idx)][numerical_columns_to_be_logged]))\n",
    "action_scaler.fit(data.loc[data.icustayid.isin(train_idx)][log_action_cols])\n",
    "\n",
    "\n",
    "limit_patients = True\n",
    "num_patients = 100\n",
    "if limit_patients:\n",
    "    train_idx = data.icustayid.unique()[:num_patients]\n",
    "print('limit_patients:',limit_patients,'len(train_idx):',len(train_idx))\n",
    "transitions_train = transition_iterator(data, idx=train_idx, scaler=scaler, log_scaler=log_scaler,  action_scaler=action_scaler, RMAX=15, log_action=True)\n",
    "print('len(transitions_train):',len(transitions_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions_dict = {k: {\n",
    "    \"s\": values[0],\n",
    "    \"a\": values[1],\n",
    "    \"r\": values[2],\n",
    "    \"s'\": values[3]   \n",
    "}\n",
    " for k, values in enumerate(transitions_train)\n",
    "}\n",
    "\n",
    "print('len(transitions_dict):',len(transitions_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute extremums of action space\n",
    "min0 = min([d['a'][0] for k, d in transitions_dict.items()])\n",
    "max0 = max([d['a'][0] for k, d in transitions_dict.items()])\n",
    "min1 = min([d['a'][1] for k, d in transitions_dict.items()])\n",
    "max1 = max([d['a'][1] for k, d in transitions_dict.items()])\n",
    "\n",
    "print('min0, max0, min1, max1:', min0, max0, min1, max1)\n",
    "\n",
    "# Also save min and max of rewards\n",
    "rmin = min([d['r'] for k, d in transitions_dict.items()])\n",
    "rmax = max([d['r'] for k, d in transitions_dict.items()])\n",
    "print('rmin, rmax:', rmin,rmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Network and PER Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q_select = ICNNBN(3, 20, 50, activation=SELU())\n",
    "Q_eval = deepcopy(Q_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CREATE PRIORITIZED ER EXPERIENCE BUFFER\n",
    "memory = PrioritizedReplayBuffer(len(transitions_dict), .6)\n",
    "n_initial_0 = 500  # this is the initial number of transitions having reward 0\n",
    "n_initial_15 = 1000  # this is the initial number of transitions having reward 15\n",
    "n_initial_m15 = 5000  # this is the initial number of transitions having reward -15\n",
    "count_15 = 0\n",
    "count_0 = 0\n",
    "count_m15 = 0\n",
    "\n",
    "saved = set()\n",
    "for idx, tr in shuffle(list(transitions_dict.items())):\n",
    "    save = False\n",
    "    if tr['r'] == 0 and count_0 < n_initial_0:\n",
    "        count_0 += 1\n",
    "        save = True\n",
    "    if tr['r'] == -15 and count_m15 < n_initial_m15:\n",
    "        count_m15 += 1\n",
    "        save = True\n",
    "    if tr['r'] == 15 and count_15 < n_initial_15:\n",
    "        count_15 += 1\n",
    "        save = True\n",
    "    \n",
    "    if save:\n",
    "        saved.add(idx)\n",
    "        # check if state is terminal\n",
    "        if tr[\"s'\"] is not None:\n",
    "            s_ = tr[\"s'\"]\n",
    "            done = False\n",
    "        else:\n",
    "            s_ = np.array(50*[np.nan])\n",
    "            done = True\n",
    "        s = tr['s']\n",
    "        a = tr['a']\n",
    "        r = np.array([tr['r']])\n",
    "\n",
    "        transition = (s,a,r,s_,done)\n",
    "        memory.add(*transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "not_present_yet = shuffle([k for k in transitions_dict if k not in saved])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clip(x):\n",
    "    \"\"\"\n",
    "    Clip val\n",
    "    ues below -RMAX or above RMAX\n",
    "    Might be useful when the target goes beyond\n",
    "    \"\"\"\n",
    "    return x*((x>=-RMAX).float())*((x<=RMAX).float()) + RMAX*(x>RMAX).float() - RMAX*(x<-RMAX).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "plt.rcParams['figure.figsize'] = (20,20)\n",
    "\n",
    "# RL parameters\n",
    "global_step = -1\n",
    "EPOCHS = 100\n",
    "gamma = .99\n",
    "max_steps_beta = 100000\n",
    "beta0 = .4\n",
    "beta = lambda t: (t<max_steps_beta)*(beta0 + t*(1-beta0)/max_steps_beta) + (t>=max_steps_beta)*1.\n",
    "RMAX = 15\n",
    "c = 1e4\n",
    "T_UPDATE = 200  # parameter to copy the weights periodically\n",
    "\n",
    "# OPTIMIZER PARAMETERS\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3  # can have higher than default value if use BatchNorm\n",
    "optimizer = t.optim.Adam(Q_select.parameters(), lr=learning_rate)\n",
    "max_steps = int(len(data) / batch_size)\n",
    "max_steps_a = 8\n",
    "\n",
    "# monitoring\n",
    "losses = []\n",
    "td_errors = []\n",
    "test_td_errors = []\n",
    "test_average_q_pred = []\n",
    "test_max_q_pred = []\n",
    "test_min_q_pred = []\n",
    "average_q_pred = []\n",
    "average_q_target = []\n",
    "max_q_pred = []\n",
    "min_q_pred = []\n",
    "max_q_target = []\n",
    "min_q_target = []\n",
    "average_vaso = []\n",
    "max_vaso = []\n",
    "min_vaso = []\n",
    "average_fluid = []\n",
    "max_fluid = []\n",
    "min_fluid = []\n",
    "\n",
    "\n",
    "# TRAIN\n",
    "for _ in range(EPOCHS):\n",
    "    t0 = time()\n",
    "    for step in range(max_steps):\n",
    "        global_step += 1\n",
    "\n",
    "        # Add transition in memory\n",
    "        idx = not_present_yet.pop()\n",
    "        tr = transitions_dict[idx]\n",
    "        if tr[\"s'\"] is not None:\n",
    "            s_ = tr[\"s'\"]\n",
    "            done = False\n",
    "        else:\n",
    "            s_ = np.array(50*[np.nan])\n",
    "            done = True\n",
    "        s = tr['s']\n",
    "        a = tr['a']\n",
    "        r = np.array([tr['r']])\n",
    "        transition = (s,a,r,s_,done)\n",
    "        memory.add(*transition)\n",
    "        \n",
    "        # Sample batch\n",
    "        s,a,r,s_,done,w,idx = memory.sample(batch_size, beta(global_step))\n",
    "        states = variable(s)\n",
    "        actions = variable(a)\n",
    "        rewards = variable(r)\n",
    "        next_states = variable(s_)\n",
    "        weights = variable(w).squeeze()\n",
    "        \n",
    "        # Init grad (set all of them to zero)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute loss\n",
    "        pred = Q_select.forward(states, actions).squeeze()  # Q(s, a)\n",
    "        Q_select.eval()\n",
    "        target, max_actions = get_q_target(Q_select, Q_eval, next_states, rewards, min0, max0, min1, max1, gamma=gamma, max_steps_a=max_steps_a, return_max_actions=True)  # max_a' Q(s', a)\n",
    "        Q_select.train()\n",
    "        target = clip(target.squeeze())\n",
    "        loss = (pred-target)\n",
    "        loss = loss**2\n",
    "        loss = loss*weights  # multiplication by the PER coefficients\n",
    "        loss = t.mean(loss) + c*t.mean(loss_beyond_RMAX(pred, RMAX))\n",
    "        \n",
    "        # Update priorities\n",
    "        td_error = t.abs(pred - target) + 1e-3\n",
    "        memory.update_priorities(idx, td_error.data.numpy())\n",
    "        \n",
    "        # Monitoring\n",
    "        losses.append(loss.data.numpy()[0])\n",
    "        td_errors.append(td_error.data.numpy().mean())\n",
    "        average_q_pred.append(t.mean(pred).squeeze().data.numpy()[0])\n",
    "        max_q_pred.append(t.max(pred).squeeze().data.numpy()[0])\n",
    "        min_q_pred.append(t.min(pred).squeeze().data.numpy()[0])\n",
    "        average_q_target.append(t.mean(target).squeeze().data.numpy()[0])\n",
    "        max_q_target.append(t.max(target).squeeze().data.numpy()[0])\n",
    "        min_q_target.append(t.min(target).squeeze().data.numpy()[0])\n",
    "        min_a = np.min(max_actions.data.numpy(), 0).squeeze()\n",
    "        max_a = np.max(max_actions.data.numpy(), 0).squeeze()\n",
    "        mean_a = np.mean(max_actions.data.numpy(), 0).squeeze()\n",
    "        min_vaso.append(min_a[0])\n",
    "        max_vaso.append(max_a[0])\n",
    "        average_vaso.append(mean_a[0])\n",
    "        min_fluid.append(min_a[1])\n",
    "        max_fluid.append(max_a[1])\n",
    "        average_fluid.append(mean_a[1])\n",
    "        \n",
    "        # Compute gradients and update weights of the selection network\n",
    "        loss.backward()\n",
    "        clip_gradients(Q_select, 10)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Keep weights positive so that the Q_select stays concave\n",
    "        Q_select.proj()\n",
    "        \n",
    "        # update parameters of the evaluation network. Its weights lag behind\n",
    "        if global_step % T_UPDATE == 0:\n",
    "            Q_eval = deepcopy(Q_select)\n",
    "            Q_eval.eval()\n",
    "\n",
    "        # PLOT\n",
    "        if step % 250 == 0:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            fig, axes = plt.subplots(3, 2)\n",
    "            # td error\n",
    "            axes[0,0].plot(moving_avg(losses))\n",
    "            axes[0,1].plot(moving_avg(td_errors))\n",
    "            # Q values (target and pred)\n",
    "            axes[1,0].plot(moving_avg(average_q_pred), label='avg pred', c='darkblue')\n",
    "            axes[1,0].plot(moving_avg(min_q_pred), label='min pred', c='darkblue', alpha=.8,linestyle=':')\n",
    "            axes[1,0].plot(moving_avg(max_q_pred), label='max pred', c='darkblue', alpha=.8,linestyle=':')\n",
    "            axes[1,0].plot(moving_avg(average_q_target), label='avg target', c='crimson')\n",
    "            axes[1,0].plot(moving_avg(min_q_target), label='min target', c='crimson', alpha=.8,linestyle=':')\n",
    "            axes[1,0].plot(moving_avg(max_q_target), label='max target', c='crimson', alpha=.8,linestyle=':')\n",
    "            axes[1,1].plot(moving_avg(average_q_pred[200:]), label='avg pred', c='darkblue')\n",
    "            axes[1,1].plot(moving_avg(min_q_pred[200:]), label='min pred', c='darkblue', alpha=.8,linestyle=':')\n",
    "            axes[1,1].plot(moving_avg(max_q_pred[200:]), label='max pred', c='darkblue', alpha=.8,linestyle=':')\n",
    "            axes[1,1].plot(moving_avg(average_q_target[200:]), label='avg target', c='crimson')\n",
    "            axes[1,1].plot(moving_avg(min_q_target[200:]), label='min target', c='crimson', alpha=.8,linestyle=':')\n",
    "            axes[1,1].plot(moving_avg(max_q_target[200:]), label='max target', c='crimson', alpha=.8,linestyle=':')\n",
    "            axes[1,1].legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "            # actions (vaso and fluid)\n",
    "            axes[2,0].plot(moving_avg(average_fluid), label='avg fluid', c='darkblue')\n",
    "            axes[2,0].plot(moving_avg(min_fluid), label='min fluid', c='darkblue', alpha=.8,linestyle=':')\n",
    "            axes[2,0].plot(moving_avg(max_fluid), label='max fluid', c='darkblue', alpha=.8,linestyle=':')\n",
    "            axes[2,0].plot(moving_avg(average_vaso), label='avg vaso', c='crimson')\n",
    "            axes[2,0].plot(moving_avg(min_vaso), label='min vaso', c='crimson', alpha=.8,linestyle=':')\n",
    "            axes[2,0].plot(moving_avg(max_vaso), label='max vaso', c='crimson', alpha=.8,linestyle=':')\n",
    "            axes[2,1].plot(moving_avg(average_fluid[200:]), label='avg fluid', c='darkblue')\n",
    "            axes[2,1].plot(moving_avg(min_fluid[200:]), label='min fluid', c='darkblue', alpha=.8,linestyle=':')\n",
    "            axes[2,1].plot(moving_avg(max_fluid[200:]), label='max fluid', c='darkblue', alpha=.8,linestyle=':')\n",
    "            axes[2,1].plot(moving_avg(average_vaso[200:]), label='avg vaso', c='crimson')\n",
    "            axes[2,1].plot(moving_avg(min_vaso[200:]), label='min vaso', c='crimson', alpha=.8,linestyle=':')\n",
    "            axes[2,1].plot(moving_avg(max_vaso[200:]), label='max vaso', c='crimson', alpha=.8,linestyle=':')\n",
    "            axes[2,1].legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print('Epoch %s\\n%s steps in this epoch\\n%s steps/s' % (str(_), str(step), str(step/(time()-t0))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate with Function A (Omer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Build parameters for the function\n",
    "discrete_transitions = np.load('trajectories.npy')\n",
    "extremums_of_action_space = (min0,max0,min1,max1)\n",
    "fence_posts = [ i for i in range(1,len(discrete_transitions)) if discrete_transitions[i][0] != discrete_transitions[i-1][0] ]\n",
    "fence_posts = fence_posts[:num_patients-1]\n",
    "print(discrete_transitions[:20])\n",
    "print(fence_posts[:20])\n",
    "states_sequence = [t[1] for t in discrete_transitions if (t[0] in train_idx)]\n",
    "actions_sequence = [t[2] for t in discrete_transitions if (t[0] in train_idx)]\n",
    "rewards_sequence = [v[2] for v in transitions_train]\n",
    "# print(transitions_train[:20])\n",
    "# rewards_sequence = [t[3] for t in discrete_transitions if (t[0] in train_idx)]\n",
    "trans_as_tuples = [(s,a,r,s_) for k,s,a,r,s_ in discrete_transitions]\n",
    "# print(rewards_sequence[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(evaluation)\n",
    "\n",
    "DR_estim, indiv_estims = evaluation.eval_ICNN_WDR_Omer(Q_select, extremums_of_action_space, states_sequence, actions_sequence, rewards_sequence, fence_posts, trans_as_tuples, gamma)\n",
    "\n",
    "print('type of DR_estim:', type(DR_estim))\n",
    "print('DR_estim:', DR_estim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(indiv_estims)\n",
    "print(DR_estim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate with Function B (HW3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Compute D, which is the same as transitions_train but as a list of lists where every element is an episode\n",
    "\n",
    "discrete_transitions = np.load('trajectories.npy')\n",
    "previous_id = discrete_transitions[0,0]\n",
    "D_train=[]\n",
    "episode = []\n",
    "for i,trans in tqdm(enumerate(discrete_transitions)):\n",
    "    if trans[0] in train_idx:\n",
    "        if trans[0] != previous_id:\n",
    "#             print('Appending ',len(episode),'-step episode from patient',previous_id, 'to D')\n",
    "            previous_id = trans[0]\n",
    "            D_train.append(deepcopy(episode))\n",
    "            episode = []   \n",
    "        # We're using rewards -15 and 15, so we need to correct the rewards that come with the\n",
    "        # file (was created with rewards -10 and 20)\n",
    "        if trans[3]:\n",
    "            r = trans[3]-5\n",
    "        else: r = trans[3]\n",
    "        episode.append((trans[1],trans[2], r, trans[4]))    \n",
    "\n",
    "print('Final length of D_train:',len(D_train)) \n",
    "print('First few episodes of D_train:',D_train[:2])\n",
    "\n",
    "## TODO: CHANGE TRANSITIONS SO THAT THE LAST STATE OF AN EPISODE IS STATE 751 IF ALIVE, 752 IF DEAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(evaluation)\n",
    "\n",
    "# DR estimator\n",
    "extremums_of_action_space = (min0,max0,min1,max1)\n",
    "DR = evaluation.eval_ICNN_DR_HW3(Q_select, extremums_of_action_space, D_train, gamma, rmin, rmax)\n",
    "print('DR:', DR)\n",
    "plt.plot(DR)\n",
    "plt.title('DR value at step', step)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate with function C (Continuous WDR implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build parameters for the function\n",
    "discrete_transitions = np.load('trajectories.npy')\n",
    "extremums_of_action_space = (min0,max0,min1,max1)\n",
    "fence_posts = [ i for i in range(1,len(discrete_transitions)) if discrete_transitions[i][0] != discrete_transitions[i-1][0] ]\n",
    "fence_posts = fence_posts[:num_patients-1]\n",
    "print('discrete_transitions[:5]',discrete_transitions[:5])\n",
    "print('fence_posts[:5]',fence_posts[:5], 'len(fence_posts):', len(fence_posts))\n",
    "cont_states_sequence = [variable(t[0]) for t in transitions_train]\n",
    "cont_actions_sequence = [variable(t[1]) for t in transitions_train]\n",
    "discrete_states = [t[1] for t in discrete_transitions if (t[0] in train_idx)]\n",
    "# discrete_actions_2 = [t[2] for t in discrete_transitions if (t[0] in train_idx)]\n",
    "discrete_actions = [t[1]['action'] for t in data.iterrows() if (t[1]['icustayid'] in train_idx)]\n",
    "rewards_sequence = [v[2] for v in transitions_train]\n",
    "print('cont_states_sequence[12:20]',cont_states_sequence[12:20])\n",
    "print('discrete_states[12:20]',discrete_states[12:20])\n",
    "# print('discrete_actions_2[:20]',discrete_actions_2[:20])\n",
    "assert(len(cont_states_sequence) == len(discrete_states))\n",
    "\n",
    "quantiles_fluid, quantiles_vaso = compute_action_quantiles(data)\n",
    "\n",
    "print(quantiles_fluid, quantiles_vaso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(evaluation)\n",
    "\n",
    "DR_estim, indiv_estim = evaluation.eval_ICNN_WDR_continuous(Q_select, extremums_of_action_space, cont_states_sequence, cont_actions_sequence, rewards_sequence, \n",
    "                             quantiles_fluid, quantiles_vaso, fence_posts, gamma)\n",
    "\n",
    "\n",
    "print('type of DR_estim:', type(DR_estim))\n",
    "print('DR_estim:', DR_estim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('Indiv_WDR_99_patients.npy', np.array(indiv_estim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loaded_indiv_estims = np.load('Indiv_WDR_99_patients.npy')\n",
    "print(loaded_indiv_estims[98])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New approach for computing integral: network for predicting int(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network for predicting the integral of the Q function\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import icnn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "importlib.reload(icnn)\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, n_layers, hidden_dim, activation = ReLU()):\n",
    "        super(Net, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.activation = activation\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        setattr(self, 'fcin', nn.Linear(input_dim, hidden_dim))\n",
    "        for l in range(self.n_layers):\n",
    "            output_dim = (l < n_layers - 1) * hidden_dim + (l == n_layers - 1) * 1\n",
    "            setattr(self, 'fc'+str(l), nn.Linear(hidden_dim, output_dim))\n",
    "            \n",
    "        self.initialize_weights_selu()\n",
    "        \n",
    "    def initialize_weights_selu(self):\n",
    "        shape = self.fcin.weight.data.numpy().shape\n",
    "        self.fcin.weight.data = t.from_numpy(np.random.normal(0, 1 / np.sqrt(shape[0]), shape)).float()\n",
    "        for l in range(self.n_layers):\n",
    "            fc = getattr(self, 'fc'+str(l))\n",
    "            shape = fc.weight.data.numpy().shape\n",
    "            fc.weight.data = t.from_numpy(np.random.normal(0, 1 / np.sqrt(shape[0]), shape)).float()\n",
    "        \n",
    "    def forward(self, z):\n",
    "#         print(\"Incoming z:\", z)\n",
    "        z_ = self.fcin(z)\n",
    "        print(\"Output z_ after first layer:\", z_)\n",
    "        z_ = self.activation(z_)\n",
    "        print(\"Output z_ after first activation:\", z_)\n",
    "        for l in range(self.n_layers):\n",
    "            fc = getattr(self, 'fc' + str(l))\n",
    "            z_ = fc(z_)\n",
    "            print(\"Output z_ after layer\",l, z_)\n",
    "            z_ = self.activation(z_)\n",
    "        return z_\n",
    "\n",
    "net = Net(len(transitions_train[1][0]),3, 20)\n",
    "print(net)\n",
    "\n",
    "params = list(net.parameters())\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Building training and test sets\n",
    "\n",
    "states_data = [trans[0] for trans in transitions_train]\n",
    "integrals = np.zeros(len(states_data))\n",
    "for i,s in enumerate(states_data):\n",
    "    integrals[i]=icnn.compute_integral(Q_select, variable(s), min0, max0,min1,max1).data.numpy()\n",
    "print(integrals[:10])\n",
    "x_train, x_test, y_train, y_test = train_test_split(states_data, integrals, test_size=0.2)    \n",
    "print('len(x_train)',len(x_train))\n",
    "print('len(y_train)',len(y_train))\n",
    "print('len(x_test)',len(x_test))\n",
    "# print('x_train[:5]',x_train[:5])\n",
    "# print('y_train[:5]',y_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# create your optimizer\n",
    "learning_rate = 1e-4\n",
    "optimizer = t.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "training_epochs = 5\n",
    "\n",
    "for e in range(training_epochs):\n",
    "    for i,s in enumerate(x_train):\n",
    "        optimizer.zero_grad()\n",
    "        print('input to net:',variable(s))\n",
    "        int_pred = net.forward(variable(s))\n",
    "        target = variable(y_train[i])  \n",
    "        net.zero_grad()\n",
    "        loss = criterion(int_pred, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         print('pred,target:',int_pred,target)\n",
    "#         print('loss',loss)\n",
    "        if i%200 == 0:\n",
    "            print('current loss:', loss.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def calculate_accuracy(y_pred, y_true, print_= False, label=''):\n",
    "    corr_labeled = sum(y_pred == y_true)\n",
    "    accuracy = corr_labeled/len(y_pred)\n",
    "    if print_:\n",
    "        print(label,'Accuracy =', accuracy*100, '% (',corr_labeled,'correctly labeled /',len(y_pred),'observations in given set )')\n",
    "    return accuracy, corr_labeled\n",
    "\n",
    "# Getting final predictions\n",
    "int_pred_train = net(variable(x_train))\n",
    "int_pred_test = net(variable(x_test))\n",
    "\n",
    "int_pred_train = int_pred_train.data.numpy().squeeze()\n",
    "int_pred_test = int_pred_test.data.numpy().squeeze()\n",
    "\n",
    "print(int_pred_train)\n",
    "print(y_train)\n",
    "\n",
    "# Print first 5 predictions vs first 5 trues:\n",
    "print('First 5 preds and trues:', int_pred_train[:5],y_train[:5])\n",
    "\n",
    "# Calculating R2 score\n",
    "print('R2 Score train:', r2_score(y_train,int_pred_train))\n",
    "print('R2 Score test:', r2_score(y_test,int_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
